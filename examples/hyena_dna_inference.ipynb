{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream prediction \n",
    "In this notebook, a HyenaDNA model is used to predict splice site acceptors with a given sequence of nucleotides.\n",
    "\n",
    "A HyenaDNA model (2 layers and width 256) is used to create embeddings of nucleotides.\n",
    "\n",
    "A small neural network is then trained, using the embeddings as inputs, to predict if a splice site acceptor is present or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bputzeys/miniforge3/envs/head_example/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Users/bputzeys/miniforge3/envs/head_example/lib/python3.11/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.13.0-rc1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Helical package to get the Hyena model\n",
    "We use a small HyenaDNA model with 2 layers and width 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "/Users/bputzeys/Documents/Helical/Code_Repos/helical/helical/models/scgpt/model_dir/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "2024-06-03, 14:52:30.751 helical.services.downloader INFO File: '/Users/bputzeys/.cache/helical/models/hyena_dna/hyenadna-tiny-1k-seqlen-d256.ckpt' exists already. File is not overwritten and nothing is downloaded.\n",
      "2024-06-03, 14:52:30.751 helical.services.downloader INFO File saved to: '/Users/bputzeys/.cache/helical/models/hyena_dna/hyenadna-tiny-1k-seqlen-d256.ckpt'\n",
      "2024-06-03, 14:52:30.875 helical.models.hyena_dna.pretrained_model INFO Loaded pretrained weights ok!\n",
      "2024-06-03, 14:52:30.878 helical.models.hyena_dna.model INFO Model finished initializing.\n"
     ]
    }
   ],
   "source": [
    "from helical.models.hyena_dna.model import HyenaDNA\n",
    "from helical.models.hyena_dna.hyena_dna_config import HyenaDNAConfig    \n",
    "configurer = HyenaDNAConfig(model_name=\"hyenadna-tiny-1k-seqlen-d256\")\n",
    "hyena_model = HyenaDNA(configurer=configurer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "Several datasets are available from the [Nucleotide Transformer](https://arxiv.org/abs/2306.15794). Using the get_dataset_config_names() function, we get a list of the available the datasets for the downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H4ac',\n",
       " 'H3K36me3',\n",
       " 'splice_sites_donors',\n",
       " 'splice_sites_acceptors',\n",
       " 'H3',\n",
       " 'H4',\n",
       " 'H3K4me3',\n",
       " 'splice_sites_all',\n",
       " 'H3K4me1',\n",
       " 'H3K14ac',\n",
       " 'enhancers_types',\n",
       " 'promoter_no_tata',\n",
       " 'H3K79me3',\n",
       " 'H3K4me2',\n",
       " 'promoter_tata',\n",
       " 'enhancers',\n",
       " 'H3K9ac',\n",
       " 'promoter_all']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "configs = get_dataset_config_names(\"InstaDeepAI/nucleotide_transformer_downstream_tasks\")\n",
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, we are interested in the splice sites acceptors for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2c55f2f36d474fb17a0ab0d079b9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632ac4a95e954cff8459071cc5d98cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/83.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185f352c4ef74c85a2d6277377b0c1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592ca4764a044e58905f73fdc7a09fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "label = \"enhancers_types\"\n",
    "dataset = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To familiarize ourselves with the data, we can print the first seqence and see if it is a splice site acceptor or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nucleotide sequence: CAGTCACATC ...\n",
      "Label name: enhancers_types and value: 0\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Nucleotide sequence:\", dataset[\"train\"][\"sequence\"][0][:10], \"...\")\n",
    "print(\"Label name:\", dataset[\"train\"].config_name, \"and value:\", dataset[\"train\"][\"label\"][0])\n",
    "num_classes = len(set(dataset[\"train\"][\"label\"]))\n",
    "print(\"Number of classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that gets the embeddings for each nucleotide sequence in the training dataset.\n",
    "\n",
    "According to the HyenaDNA [paper](https://arxiv.org/pdf/2306.15794): \"[they] average across the tokens to obtain a single classification token\".\n",
    "\n",
    "In our code below, the Hyena model returns a (602, 256) matrix. We average column wise resulting in a vector of shape (256, ) for each observation.\n",
    "\n",
    "During the training process, we also found that it is beneficial to normalize the data row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs(dataset: DatasetDict, ratio: float = 1.0):\n",
    "    \n",
    "    x = np.empty((0, configurer.config['d_model'])) \n",
    "    labels = np.empty((0,), dtype=int)\n",
    "\n",
    "    # disable logging to avoid cluttering the output\n",
    "    import logging\n",
    "    logging.disable(logging.CRITICAL)\n",
    "\n",
    "    # use tqdm for a progress bar\n",
    "    length = int(len(dataset)*ratio)\n",
    "    for i in tqdm(range(length)):\n",
    "        sequence = dataset[\"sequence\"][i]\n",
    "        \n",
    "        tokenized_sequence = hyena_model.process_data(sequence)\n",
    "        embeddings = hyena_model.get_embeddings(tokenized_sequence)\n",
    "        \n",
    "        numpy_array = embeddings[0].detach().numpy()\n",
    "        mean_array = numpy_array.mean(axis=0)\n",
    "        x = np.append(x, [mean_array], axis=0)\n",
    "        \n",
    "    # normalize the data\n",
    "    x = (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "    labels = np.array(dataset[\"label\"][:length])\n",
    "    return x, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are a high number of rows, and thus lots of inferences with the Hyena model, it is beneficial to do this step once and save the output in a `.npy` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 108/14968 [00:03<07:01, 35.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_norm_256\u001b[39m\u001b[38;5;124m\"\u001b[39m, x)\n\u001b[1;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_norm_256\u001b[39m\u001b[38;5;124m\"\u001b[39m, y)\n",
      "Cell \u001b[0;32mIn[136], line 16\u001b[0m, in \u001b[0;36mget_model_inputs\u001b[0;34m(dataset, ratio)\u001b[0m\n\u001b[1;32m     13\u001b[0m sequence \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n\u001b[1;32m     15\u001b[0m tokenized_sequence \u001b[38;5;241m=\u001b[39m hyena_model\u001b[38;5;241m.\u001b[39mprocess_data(sequence)\n\u001b[0;32m---> 16\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mhyena_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m numpy_array \u001b[38;5;241m=\u001b[39m embeddings[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     19\u001b[0m mean_array \u001b[38;5;241m=\u001b[39m numpy_array\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Helical/Code_Repos/helical/helical/models/hyena_dna/model.py:103\u001b[0m, in \u001b[0;36mHyenaDNA.get_embeddings\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    101\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Helical/Code_Repos/helical/helical/models/hyena_dna/standalone_hyenadna.py:914\u001b[0m, in \u001b[0;36mHyenaDNAModel.forward\u001b[0;34m(self, input_ids, position_ids, state)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, position_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m): \u001b[38;5;66;03m# state for the repo interface\u001b[39;00m\n\u001b[0;32m--> 914\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_head:\n\u001b[1;32m    917\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(hidden_states)\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Helical/Code_Repos/helical/helical/models/hyena_dna/standalone_hyenadna.py:728\u001b[0m, in \u001b[0;36mLMBackbone.forward\u001b[0;34m(self, input_ids, position_ids)\u001b[0m\n\u001b[1;32m    725\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 728\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m dropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_f(hidden_states)\n\u001b[1;32m    731\u001b[0m residual \u001b[38;5;241m=\u001b[39m (dropped \u001b[38;5;241m+\u001b[39m residual) \u001b[38;5;28;01mif\u001b[39;00m residual \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dropped\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Helical/Code_Repos/helical/helical/models/hyena_dna/standalone_hyenadna.py:530\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, hidden_states, residual, mixer_subset, mixer_kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mixer_subset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     mixer_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmixer_subset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m mixer_subset\n\u001b[0;32m--> 530\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmixer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mixer_subset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     residual \u001b[38;5;241m=\u001b[39m residual[:, mixer_subset]\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Helical/Code_Repos/helical/helical/models/hyena_dna/standalone_hyenadna.py:279\u001b[0m, in \u001b[0;36mHyenaOperator.forward\u001b[0;34m(self, u, *args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj(u)\n\u001b[1;32m    277\u001b[0m u \u001b[38;5;241m=\u001b[39m rearrange(u, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb l d -> b d l\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 279\u001b[0m uc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshort_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,:l_filter]\n\u001b[1;32m    280\u001b[0m \u001b[38;5;241m*\u001b[39mx, v \u001b[38;5;241m=\u001b[39m uc\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    282\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_fn\u001b[38;5;241m.\u001b[39mfilter(l_filter)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/head_example/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x, y = get_model_inputs(dataset[\"train\"], 1)\n",
    "np.save(f\"x_{label}_norm_256\", x)\n",
    "np.save(f\"y_{label}_norm_256\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and one-hot-encode the labels.\n",
    "\n",
    "We split the training set into actual training data and a test set. \n",
    "\n",
    "This is optional and the entire dataset can be used for training. We did this to avoid data leakage by not touching the test set during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(f\"x_{label}_norm_256.npy\")\n",
    "y = np.load(f\"y_{label}_norm_256.npy\")\n",
    "\n",
    "# # One-hot encode the labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "y_encoded = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82370 (321.76 KB)\n",
      "Trainable params: 82370 (321.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.3697 - f1_score: 0.8396 - val_loss: 0.2161 - val_f1_score: 0.9237\n",
      "Epoch 2/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.2482 - f1_score: 0.9023 - val_loss: 0.2098 - val_f1_score: 0.9273\n",
      "Epoch 3/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.2204 - f1_score: 0.9124 - val_loss: 0.1996 - val_f1_score: 0.9273\n",
      "Epoch 4/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.2075 - f1_score: 0.9187 - val_loss: 0.1959 - val_f1_score: 0.9274\n",
      "Epoch 5/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1861 - f1_score: 0.9265 - val_loss: 0.2025 - val_f1_score: 0.9200\n",
      "Epoch 6/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1803 - f1_score: 0.9292 - val_loss: 0.1959 - val_f1_score: 0.9346\n",
      "Epoch 7/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1730 - f1_score: 0.9310 - val_loss: 0.1842 - val_f1_score: 0.9364\n",
      "Epoch 8/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1694 - f1_score: 0.9340 - val_loss: 0.1949 - val_f1_score: 0.9218\n",
      "Epoch 9/25\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.1656 - f1_score: 0.9372 - val_loss: 0.1888 - val_f1_score: 0.9237\n",
      "Epoch 10/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1631 - f1_score: 0.9352 - val_loss: 0.1977 - val_f1_score: 0.9237\n",
      "Epoch 11/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1499 - f1_score: 0.9427 - val_loss: 0.1855 - val_f1_score: 0.9310\n",
      "Epoch 12/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1505 - f1_score: 0.9409 - val_loss: 0.1957 - val_f1_score: 0.9327\n",
      "Epoch 13/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1454 - f1_score: 0.9433 - val_loss: 0.1916 - val_f1_score: 0.9364\n",
      "Epoch 14/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1308 - f1_score: 0.9498 - val_loss: 0.1972 - val_f1_score: 0.9346\n",
      "Epoch 15/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1371 - f1_score: 0.9445 - val_loss: 0.2034 - val_f1_score: 0.9310\n",
      "Epoch 16/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1270 - f1_score: 0.9510 - val_loss: 0.2136 - val_f1_score: 0.9182\n",
      "Epoch 17/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1277 - f1_score: 0.9524 - val_loss: 0.1904 - val_f1_score: 0.9219\n",
      "Epoch 18/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1147 - f1_score: 0.9556 - val_loss: 0.2047 - val_f1_score: 0.9219\n",
      "Epoch 19/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1215 - f1_score: 0.9526 - val_loss: 0.1925 - val_f1_score: 0.9218\n",
      "Epoch 20/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1172 - f1_score: 0.9487 - val_loss: 0.1974 - val_f1_score: 0.9237\n",
      "Epoch 21/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1098 - f1_score: 0.9574 - val_loss: 0.2113 - val_f1_score: 0.9364\n",
      "Epoch 22/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1127 - f1_score: 0.9534 - val_loss: 0.2038 - val_f1_score: 0.9253\n",
      "Epoch 23/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1029 - f1_score: 0.9574 - val_loss: 0.2139 - val_f1_score: 0.9273\n",
      "Epoch 24/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1055 - f1_score: 0.9609 - val_loss: 0.2086 - val_f1_score: 0.9310\n",
      "Epoch 25/25\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0996 - f1_score: 0.9592 - val_loss: 0.2178 - val_f1_score: 0.9273\n"
     ]
    }
   ],
   "source": [
    "input_shape = (configurer.config['d_model'],)\n",
    "\n",
    "# define the model\n",
    "head_model = Sequential()\n",
    "\n",
    "head_model.add(Dense(256, activation='relu', input_shape=input_shape))\n",
    "head_model.add(Dropout(0.4)) \n",
    "head_model.add(Dense(64, activation='relu'))\n",
    "head_model.add(Dropout(0.4))  \n",
    "head_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "f1_score = F1Score(num_classes, average='macro')\n",
    "\n",
    "head_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=f1_score)\n",
    "head_model.summary()\n",
    "\n",
    "# Setup callbacks\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "history = head_model.fit(X_train, y_train, epochs=25, batch_size=64, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 621/621 [00:15<00:00, 39.24it/s]\n"
     ]
    }
   ],
   "source": [
    "X_unseen, y_unseen = get_model_inputs(dataset[\"test\"], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 339us/step\n",
      "Correct predictions: 92.11%\n"
     ]
    }
   ],
   "source": [
    "predictions_nn = head_model.predict(X_unseen)\n",
    "\n",
    "y_pred = np.argmax(predictions_nn, axis=1)\n",
    "print(\"Correct predictions: {:.2f}%\".format(sum(np.equal(y_pred, y_unseen))*100/len(y_unseen)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Hyena](https://arxiv.org/pdf/2306.15794) and the [Nucleotide transformer](https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1.full.pdf) papers, report accuracies around 95% for this task. Our results clearly underperform in comparison. This is probably due to the much larger models being used for the NT, while the Hyena model was re-trained from scratch for this task. In future work, we want to achieve these accuracies too with either approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we also trained an SVM model and obtained similar results (to our small NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the SVM model\n",
    "svm_model = svm.SVC(kernel='rbf', degree=3, C=1, decision_function_shape='ovr')  # One-vs-rest strategy\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 91.9%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "unseen_predictions_svm = svm_model.predict(X_unseen)\n",
    "\n",
    "accuracy = accuracy_score(y_unseen, unseen_predictions_svm)\n",
    "print(\"Test accuracy: {:.1f}%\".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# NN\n",
    "# y_true = np.argmax(y_test, axis=1)\n",
    "# y_pred_classes = np.argmax(predictions_nn, axis=1)\n",
    "\n",
    "# SVM\n",
    "y_true = y_test\n",
    "y_pred_classes = svm_model.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helical-package",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
