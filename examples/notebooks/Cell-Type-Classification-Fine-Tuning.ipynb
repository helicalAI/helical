{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification fine-tuning using Helical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell type classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.models.geneformer.geneformer_config import GeneformerConfig\n",
    "from helical.models.geneformer.fine_tuning_model import GeneformerFineTuningModel\n",
    "from helical.models.geneformer.model import Geneformer\n",
    "from helical.models.scgpt.fine_tuning_model import scGPTFineTuningModel\n",
    "from helical.models.scgpt.model import scGPT,scGPTConfig\n",
    "from helical.models.uce.model import UCE, UCEConfig\n",
    "from helical.models.uce.fine_tuning_model import UCEFineTuningModel\n",
    "from helical.utils.dataset_to_anndata import get_anndata_from_hf_dataset\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"helical-ai/yolksac_human\",trust_remote_code=True, download_mode=\"reuse_cache_if_exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_anndata_from_hf_dataset(ds[\"train\"])\n",
    "test_dataset = get_anndata_from_hf_dataset(ds[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this classification task we want to predict cell type classes\n",
    "- So we save the cell types as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types_train = list(np.array(train_dataset.obs[\"LVL1\"].tolist()))\n",
    "cell_types_test = list(np.array(test_dataset.obs[\"LVL1\"].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We convert these string labels into unique integer classes for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = set(cell_types_train)\n",
    "class_id_dict = dict(zip(label_set, [i for i in range(len(label_set))]))\n",
    "\n",
    "for i in range(len(cell_types_train)):\n",
    "    cell_types_train[i] = class_id_dict[cell_types_train[i]]\n",
    "\n",
    "for i in range(len(cell_types_test)):\n",
    "    cell_types_test[i] = class_id_dict[cell_types_test[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geneformer Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the desired pretrained Geneformer model and desired configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneformer_config = GeneformerConfig(device=device, batch_size=5, model_name=\"gf-6L-30M-i2048\")\n",
    "geneformer = Geneformer(configurer = geneformer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data so it is in the correct form for Geneformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneformer_train_dataset = geneformer.process_data(train_dataset)\n",
    "geneformer_test_dataset = geneformer.process_data(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geneformer makes use of the Hugging Face dataset class and so we need to add the labels as a column to this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneformer_train_dataset = geneformer_train_dataset.add_column(\"LVL1\", cell_types_train)\n",
    "geneformer_test_dataset = geneformer_test_dataset.add_column(\"LVL1\", cell_types_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Geneformer Fine-Tuning Model from the Helical package which appends a fine-tuning head automatically from the list of available heads\n",
    "- Define the task type, which in this case is classification\n",
    "- Defined the output size, which is the number of unique labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneformer_fine_tune = GeneformerFineTuningModel(geneformer_model=geneformer, fine_tuning_head=\"classification\", output_size=len(label_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.geneformer.fine_tuning_model:Freezing the first 2 encoder layers of the Geneformer model during fine-tuning.\n",
      "INFO:helical.models.geneformer.fine_tuning_model:Starting Fine-Tuning\n",
      "Fine-Tuning: epoch 1/1: 100%|██████████| 5069/5069 [06:17<00:00, 13.44it/s, loss=0.061] \n",
      "Fine-Tuning Validation: 100%|██████████| 1268/1268 [00:44<00:00, 28.38it/s, accuracy=0.99]\n",
      "INFO:helical.models.geneformer.fine_tuning_model:Fine-Tuning Complete. Epochs: 1\n"
     ]
    }
   ],
   "source": [
    "geneformer_fine_tune.train(train_dataset=geneformer_train_dataset, validation_dataset=geneformer_test_dataset, label=\"LVL1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scGPT Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same procedure with scGPT\n",
    "- Loading the model and setting desired configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scgpt_config=scGPTConfig(batch_size=10, device=device)\n",
    "scgpt = scGPT(configurer=scgpt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly different methodology for getting the dataset for scGPT since it does not make use of the Hugging Face Dataset class\n",
    "- Split the data into a train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.scgpt.model:Filtering out 11163 genes to a total of 26155 genes with an id in the scGPT vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.scgpt.model:Filtering out 11163 genes to a total of 26155 genes with an id in the scGPT vocabulary.\n"
     ]
    }
   ],
   "source": [
    "dataset = scgpt.process_data(train_dataset, gene_names = \"gene_name\")\n",
    "validation_dataset = scgpt.process_data(test_dataset, gene_names = \"gene_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the scGPT fine-tuning model with the desired head and number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scgpt_fine_tune = scGPTFineTuningModel(scGPT_model=scgpt, fine_tuning_head=\"classification\", output_size=len(label_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scGPT fine tuning we have to pass in the labels as a separate list\n",
    "- This is the same for the validation and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.scgpt.fine_tuning_model:Starting Fine-Tuning\n",
      "Fine-Tuning: epoch 1/1: 100%|██████████| 2535/2535 [02:03<00:00, 20.52it/s, loss=0.227]\n",
      "Fine-Tuning Validation: 100%|██████████| 634/634 [00:10<00:00, 59.94it/s, accuracy=0.986]\n",
      "INFO:helical.models.scgpt.fine_tuning_model:Fine-Tuning Complete. Epochs: 1\n"
     ]
    }
   ],
   "source": [
    "scgpt_fine_tune.train(train_input_data=dataset, train_labels=cell_types_train, validation_input_data=validation_dataset, validation_labels=cell_types_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCE Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uce_config=UCEConfig(batch_size=5, device=device)\n",
    "uce = UCE(configurer=uce_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data the same way as for scGPT\n",
    "- Add names for each dataset, as datasets are stored as .npz files and separate files are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = uce.process_data(train_dataset, name=\"train\", gene_names=\"gene_name\")\n",
    "validation_dataset = uce.process_data(test_dataset, name=\"validation\", gene_names=\"gene_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "uce_fine_tune = UCEFineTuningModel(uce_model=uce, fine_tuning_head=\"classification\", output_size=len(label_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.uce.fine_tuning_model:Starting Fine-Tuning\n",
      "Fine-Tuning: epoch 1/1:   0%|          | 1/5069 [00:00<14:38,  5.77it/s, loss=1.79]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning: epoch 1/1:  72%|███████▏  | 3649/5069 [09:17<03:36,  6.56it/s, loss=1.14]"
     ]
    }
   ],
   "source": [
    "uce_fine_tune.train(train_input_data=dataset, train_labels=cell_types_train, validation_input_data=validation_dataset, validation_labels=cell_types_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helical-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
