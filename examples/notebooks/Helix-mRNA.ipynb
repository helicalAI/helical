{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Helix-mRNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the Helical GitHub [here](https://github.com/helicalAI)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook we will dive into using our latest mRNA Bio Foundation Model, Helix-mRNA.**\n",
    "\n",
    "**We will get and plot embeddings for our data.**\n",
    "\n",
    "**We will fine-tune the model both using the Helical package**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on a CUDA device compatible with mamba-ssm and causal-conv1d install the package below, otherwise remove the [mamba-ssm] optional dependency\n",
    "- If running on colab, remove the [mamba-ssm] dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade helical[mamba-ssm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.models.helix_mrna import HelixmRNAConfig, HelixmRNA, HelixmRNAFineTuningModel\n",
    "import subprocess\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download one of CodonBERT's fine-tuning benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/Sanofi-Public/CodonBERT/refs/heads/master/benchmarks/CodonBERT/data/fine-tune/mRFP_Expression.csv\"\n",
    "\n",
    "output_filename = \"mRFP_Expression.csv\"\n",
    "wget_command = [\"wget\", \"-O\", output_filename, url]\n",
    "\n",
    "try:\n",
    "    subprocess.run(wget_command, check=True)\n",
    "    print(f\"File downloaded successfully as {output_filename}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset as a pandas dataframe and get the splits\n",
    "- For this example we take a subset of the splits, feel free to run it on the entire dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(output_filename)\n",
    "train_data = dataset[dataset[\"Split\"] == \"train\"][:10]\n",
    "eval_data = dataset[dataset[\"Split\"] == \"val\"][:5]\n",
    "test_data = dataset[dataset[\"Split\"] == \"test\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our Helix-mRNA model and desired configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the max length to the maximum length of the sequences in the training data + 10 to include space for special tokens\n",
    "helix_mrna_config = HelixmRNAConfig(device=device, batch_size=1, max_length=max(len(s) for s in train_data[\"Sequence\"])+10)\n",
    "helix_mrna = HelixmRNA(helix_mrna_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process our training sequences to tokenize them and prepare them for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_data = helix_mrna.process_data(train_data[\"Sequence\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get an embeddings for each letter/token in the sequence, in this case 100 embeddings for each of the 688 tokens and our embedding dimension is 256\n",
    "- Because the model has a recurrent nature, our final non-special token embedding at the second last position encapsulates everything that came before it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = helix_mrna.get_embeddings(processed_train_data)\n",
    "embeddings = embeddings[:, -2, :]\n",
    "print(embeddings.shape)\n",
    "print(embeddings[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the model on our data\n",
    "- This is a regression task and so our output is 1 continuous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helix_mrna_fine_tuning_model = HelixmRNAFineTuningModel(helix_mrna_config=helix_mrna_config, fine_tuning_head=\"regression\", output_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our training data is already processed since the standard Helix-mRNA model and fine-tuning model take the same input!\n",
    "- We process our eval and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_eval_data = helix_mrna_fine_tuning_model.process_data(eval_data[\"Sequence\"].to_list())\n",
    "processed_test_data = helix_mrna_fine_tuning_model.process_data(test_data[\"Sequence\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run fine-tuning on the model for this small sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helix_mrna_fine_tuning_model.train(train_dataset=processed_train_data, \n",
    "                                   train_labels=train_data[\"Value\"].to_numpy().reshape(-1, 1),\n",
    "                                   validation_dataset=processed_eval_data, \n",
    "                                   validation_labels= eval_data[\"Value\"].to_numpy().reshape(-1, 1),\n",
    "                                   epochs=5,\n",
    "                                   loss_function=torch.nn.MSELoss(),\n",
    "                                   trainable_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get outputs from our model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = helix_mrna_fine_tuning_model.get_outputs(processed_test_data)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
