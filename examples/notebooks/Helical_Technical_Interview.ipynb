{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8446fdb8ef4f420082bde10affa2f9f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85284b02a2d447458aef5f612b17f7b6",
              "IPY_MODEL_b192bf5d563f4030904607a336ac3653",
              "IPY_MODEL_8b98bebd04924eb3ad29a84adba6a66c"
            ],
            "layout": "IPY_MODEL_df793411605d440ba554fc74304487dc"
          }
        },
        "85284b02a2d447458aef5f612b17f7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b5f6e5a86e94ce6b1d18a00cac7fbb5",
            "placeholder": "​",
            "style": "IPY_MODEL_5d5b9e7aa46847ddbf6d4491762437ea",
            "value": "README.md: "
          }
        },
        "b192bf5d563f4030904607a336ac3653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dbd066a0c3342dfad499afd906d0505",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f8b9ac89ebf444da0c7d6459ebd8366",
            "value": 1
          }
        },
        "8b98bebd04924eb3ad29a84adba6a66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_443785393669410cbf834dcfe492ef13",
            "placeholder": "​",
            "style": "IPY_MODEL_fb61ccfcbb3543aa9cb93cf6ae49e5ea",
            "value": " 4.23k/? [00:00&lt;00:00, 507kB/s]"
          }
        },
        "df793411605d440ba554fc74304487dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b5f6e5a86e94ce6b1d18a00cac7fbb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d5b9e7aa46847ddbf6d4491762437ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dbd066a0c3342dfad499afd906d0505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7f8b9ac89ebf444da0c7d6459ebd8366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "443785393669410cbf834dcfe492ef13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb61ccfcbb3543aa9cb93cf6ae49e5ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa83523168e84dfd9e20121457fff2fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67d9f9cdf7bf4a27a1e217632148750a",
              "IPY_MODEL_68a6dcc9aae24a3b8d6d943d4795f8c6",
              "IPY_MODEL_edba8ac4fc5f47ea9e1322aab8ac67f0"
            ],
            "layout": "IPY_MODEL_dd745813bb1348db9487b5016adc30e5"
          }
        },
        "67d9f9cdf7bf4a27a1e217632148750a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40fd804e49064c519f6e0942a60f79d5",
            "placeholder": "​",
            "style": "IPY_MODEL_9a0302ee767542308338ad92df62c2bf",
            "value": "yolksac_human.py: "
          }
        },
        "68a6dcc9aae24a3b8d6d943d4795f8c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abbb02fdc0e0475190101989443b2356",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01c71441dc554f92bb8df4113316afde",
            "value": 1
          }
        },
        "edba8ac4fc5f47ea9e1322aab8ac67f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95441cecd33f4f018fb0a1b2846f096c",
            "placeholder": "​",
            "style": "IPY_MODEL_743a890b7dd845d3a623c4bc26695c89",
            "value": " 4.20k/? [00:00&lt;00:00, 511kB/s]"
          }
        },
        "dd745813bb1348db9487b5016adc30e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40fd804e49064c519f6e0942a60f79d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a0302ee767542308338ad92df62c2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abbb02fdc0e0475190101989443b2356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "01c71441dc554f92bb8df4113316afde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95441cecd33f4f018fb0a1b2846f096c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "743a890b7dd845d3a623c4bc26695c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "234e3040931d4b4aa766496aac47612b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a6f2e5eefe44ef09ee506b87c123c57",
              "IPY_MODEL_738490a45bed4c61ac8f9704db4d8a3a",
              "IPY_MODEL_e7a966cc27524c768da2cd18947250b3"
            ],
            "layout": "IPY_MODEL_17ca5fa83ac142ab8f8e2b4c436729e5"
          }
        },
        "2a6f2e5eefe44ef09ee506b87c123c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aa3621b4bea424499eb17526f46e378",
            "placeholder": "​",
            "style": "IPY_MODEL_2478c9a3a23940a88662e499d1cac968",
            "value": "./data/17_04_24_YolkSacRaw_F158_WE_annot(…): 100%"
          }
        },
        "738490a45bed4c61ac8f9704db4d8a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67775ca4db434d109371da83101efa55",
            "max": 552929816,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc39b9dc4c884434a63a826f6524645c",
            "value": 552929816
          }
        },
        "e7a966cc27524c768da2cd18947250b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f599166671f40e4b04a0a38e47ee472",
            "placeholder": "​",
            "style": "IPY_MODEL_fb329a486ab4422aa3f0f3477c1898f9",
            "value": " 553M/553M [00:01&lt;00:00, 412MB/s]"
          }
        },
        "17ca5fa83ac142ab8f8e2b4c436729e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aa3621b4bea424499eb17526f46e378": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2478c9a3a23940a88662e499d1cac968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67775ca4db434d109371da83101efa55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc39b9dc4c884434a63a826f6524645c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f599166671f40e4b04a0a38e47ee472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb329a486ab4422aa3f0f3477c1898f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "219dcfb840ef4b91ab3176df102b4a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f839394c10454990b146d77c8525ab60",
              "IPY_MODEL_4164d7eb115f4e0ca935d58e1af69d06",
              "IPY_MODEL_59b5082b1e434c6591460d7519f2fa26"
            ],
            "layout": "IPY_MODEL_9549048c665948d6ad26518a8cd85ee3"
          }
        },
        "f839394c10454990b146d77c8525ab60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1604291be4124740a286e5bafe3923dd",
            "placeholder": "​",
            "style": "IPY_MODEL_ae9c8a084fb74baca920eab4c872162b",
            "value": "Generating train split: "
          }
        },
        "4164d7eb115f4e0ca935d58e1af69d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ad14e542e27445484da63c9bb936b6c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_826ce1c9896444469527eabdfec748de",
            "value": 1
          }
        },
        "59b5082b1e434c6591460d7519f2fa26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05cd9140a4c14a9e83977f31bd4e34d4",
            "placeholder": "​",
            "style": "IPY_MODEL_f5415ca3424d4ff2a1adae01f891b3a0",
            "value": " 25344/0 [00:23&lt;00:00, 873.24 examples/s]"
          }
        },
        "9549048c665948d6ad26518a8cd85ee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1604291be4124740a286e5bafe3923dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae9c8a084fb74baca920eab4c872162b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ad14e542e27445484da63c9bb936b6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "826ce1c9896444469527eabdfec748de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05cd9140a4c14a9e83977f31bd4e34d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5415ca3424d4ff2a1adae01f891b3a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1e73efb8cb14d048929a7105a5c2253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83116df8400548f19302e1cec3a5960a",
              "IPY_MODEL_f0f600b6bd464c59873bf3af7c69f7d4",
              "IPY_MODEL_a3442e98f73b46a99ff86a4e6e9788bf"
            ],
            "layout": "IPY_MODEL_f1f39b53f2e34b309ddcfbdf4431883c"
          }
        },
        "83116df8400548f19302e1cec3a5960a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df9b225cfa224035b52020c4e6b10cf6",
            "placeholder": "​",
            "style": "IPY_MODEL_c2dc53760ae543d5b3c2a2e8098bf31f",
            "value": "Generating test split: "
          }
        },
        "f0f600b6bd464c59873bf3af7c69f7d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45bdcd35181f40589ede5b4336dfdb90",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9c56e7d76b2485c89138179512d3856",
            "value": 1
          }
        },
        "a3442e98f73b46a99ff86a4e6e9788bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b9f9e47565e4d5a868eb0b99f845feb",
            "placeholder": "​",
            "style": "IPY_MODEL_acbb653117e94dd3953160587ef40ff3",
            "value": " 6336/0 [00:05&lt;00:00, 1098.86 examples/s]"
          }
        },
        "f1f39b53f2e34b309ddcfbdf4431883c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df9b225cfa224035b52020c4e6b10cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2dc53760ae543d5b3c2a2e8098bf31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45bdcd35181f40589ede5b4336dfdb90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c9c56e7d76b2485c89138179512d3856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b9f9e47565e4d5a868eb0b99f845feb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acbb653117e94dd3953160587ef40ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Helical Technical Assessment</u>**"
      ],
      "metadata": {
        "id": "yN0XkOlOVkam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Abstract </u>**\n",
        "\n",
        "- The aim of this task was to take one model in the helical package to run more efficiently over a large amount of perturbations (i.e. inference). For this technical task, we have taken the Geneformer model.\n",
        "\n",
        "- We then leverged FP16 (soft quantization), mixed precision and batching to optimize for runtime, GPU/CPU utilization, and memory use. We ensured these results remained comparable to baseline outputs. We also calculated the K-S Statistics and the P-Values between the results from the baseline experiment and the experients consisting of optimization strategies to ensure that the results remain comparable.\n",
        "\n",
        "- We concluded that batching provided the largest inference speedup (~3×), outperforming both FP16 casting and AMP. GPU utilization did not correlate directly with latency, highlighting kernel efficiency and launch amortization as dominant factors. FP16 (soft quantization) primarily reduced memory footprint, while batching dominated throughput gains."
      ],
      "metadata": {
        "id": "iDhhQeylV-Wd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<u>Requirements</u>**\n",
        "- Before we get started, it's pivotal that we install the following packages. Otherwise, the following steps won't work."
      ],
      "metadata": {
        "id": "2RSjBunqqwsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RkD-ZJAIb8IF",
        "outputId": "b3c45663-cf59-4c3a-b69b-f0f61bcd1199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting helical\n",
            "  Downloading helical-1.5.3-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==1.4.0 (from helical)\n",
            "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting anndata>=0.11 (from helical)\n",
            "  Downloading anndata-0.12.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting bitsandbytes>=0.48.2 (from helical)\n",
            "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting datasets==3.6.0 (from helical)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: einops==0.8.1 in /usr/local/lib/python3.12/dist-packages (from helical) (0.8.1)\n",
            "Collecting gitpython==3.1.44 (from helical)\n",
            "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting hydra-core==1.3.2 (from helical)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting loompy==3.0.7 (from helical)\n",
            "  Downloading loompy-3.0.7.tar.gz (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting louvain==0.8.2 (from helical)\n",
            "  Downloading louvain-0.8.2.tar.gz (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy<2.3,>=2.1.3 (from helical)\n",
            "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: omegaconf==2.3.0 in /usr/local/lib/python3.12/dist-packages (from helical) (2.3.0)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (from helical) (2.2.2)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from helical) (5.29.5)\n",
            "Collecting pybiomart (from helical)\n",
            "  Downloading pybiomart-0.2.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from helical) (2.32.4)\n",
            "Collecting scib==1.1.5 (from helical)\n",
            "  Downloading scib-1.1.5-1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from helical) (1.6.1)\n",
            "Collecting scikit-misc>=0.5.2 (from helical)\n",
            "  Downloading scikit_misc-0.5.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting scipy==1.13.1 (from helical)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from helical) (0.2.1)\n",
            "Collecting torch==2.7.0 (from helical)\n",
            "  Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting transformers<=4.51.3,>=4.49.0 (from helical)\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.4.0->helical) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.4.0->helical) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==1.4.0->helical) (6.0.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.4.0->helical) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.4.0->helical) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->helical) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->helical) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->helical) (0.3.8)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->helical) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->helical) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->helical) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->helical) (2025.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython==3.1.44->helical) (4.0.12)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core==1.3.2->helical) (4.9.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from loompy==3.0.7->helical) (3.15.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from loompy==3.0.7->helical) (75.2.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from loompy==3.0.7->helical) (0.60.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from loompy==3.0.7->helical) (8.3.1)\n",
            "Collecting numpy-groupies (from loompy==3.0.7->helical)\n",
            "  Downloading numpy_groupies-0.11.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting igraph<0.12,>=0.10.0 (from louvain==0.8.2->helical)\n",
            "  Downloading igraph-0.11.9-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2->helical) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2->helical) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2->helical) (2025.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from scib==1.1.5->helical) (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from scib==1.1.5->helical) (3.10.0)\n",
            "Collecting scanpy>=1.5 (from scib==1.1.5->helical)\n",
            "  Downloading scanpy-1.11.5-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting leidenalg (from scib==1.1.5->helical)\n",
            "  Downloading leidenalg-0.11.0-cp38-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.12/dist-packages (from scib==1.1.5->helical) (0.5.9.post2)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.12/dist-packages (from scib==1.1.5->helical) (4.0.1)\n",
            "Requirement already satisfied: llvmlite in /usr/local/lib/python3.12/dist-packages (from scib==1.1.5->helical) (0.43.0)\n",
            "Collecting deprecated (from scib==1.1.5->helical)\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (12.6.80)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->helical)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (12.5.4.2)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->helical)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->helical)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->helical) (1.11.1.6)\n",
            "Collecting triton==3.3.0 (from torch==2.7.0->helical)\n",
            "  Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting array-api-compat>=1.7.1 (from anndata>=0.11->helical)\n",
            "  Downloading array_api_compat-1.13.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting legacy-api-wrap (from anndata>=0.11->helical)\n",
            "  Downloading legacy_api_wrap-1.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.12/dist-packages (from anndata>=0.11->helical) (8.4.0)\n",
            "Collecting zarr!=3.0.*,>=2.18.7 (from anndata>=0.11->helical)\n",
            "  Downloading zarr-3.1.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->helical) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->helical) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->helical) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->helical) (2025.11.12)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.5.0->helical) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.5.0->helical) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.51.3,>=4.49.0->helical) (2025.11.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<=4.51.3,>=4.49.0->helical)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from pybiomart->helical) (1.0.0)\n",
            "Collecting requests-cache (from pybiomart->helical)\n",
            "  Downloading requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->helical) (3.13.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython==3.1.44->helical) (5.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.4.0->helical) (1.2.0)\n",
            "Collecting texttable>=1.6.2 (from igraph<0.12,>=0.10.0->louvain==0.8.2->helical)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2->helical) (1.17.0)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.12/dist-packages (from scanpy>=1.5->scib==1.1.5->helical) (1.0.2)\n",
            "Requirement already satisfied: pynndescent>=0.5.13 in /usr/local/lib/python3.12/dist-packages (from scanpy>=1.5->scib==1.1.5->helical) (0.5.13)\n",
            "Collecting session-info2 (from scanpy>=1.5->scib==1.1.5->helical)\n",
            "  Downloading session_info2-0.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: statsmodels>=0.14.5 in /usr/local/lib/python3.12/dist-packages (from scanpy>=1.5->scib==1.1.5->helical) (0.14.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scib==1.1.5->helical) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scib==1.1.5->helical) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scib==1.1.5->helical) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scib==1.1.5->helical) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scib==1.1.5->helical) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scib==1.1.5->helical) (3.2.5)\n",
            "INFO: pip is looking at multiple versions of numba to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting numba (from loompy==3.0.7->helical)\n",
            "  Downloading numba-0.63.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting llvmlite (from scib==1.1.5->helical)\n",
            "  Downloading llvmlite-0.46.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.7.0->helical) (1.3.0)\n",
            "Collecting donfig>=0.8 (from zarr!=3.0.*,>=2.18.7->anndata>=0.11->helical)\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata>=0.11->helical) (1.7.1)\n",
            "Collecting numcodecs>=0.14 (from zarr!=3.0.*,>=2.18.7->anndata>=0.11->helical)\n",
            "  Downloading numcodecs-0.16.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated->scib==1.1.5->helical) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.7.0->helical) (3.0.3)\n",
            "INFO: pip is looking at multiple versions of leidenalg to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting leidenalg (from scib==1.1.5->helical)\n",
            "  Downloading leidenalg-0.10.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.12/dist-packages (from requests-cache->pybiomart->helical) (25.4.0)\n",
            "Collecting cattrs>=22.2 (from requests-cache->pybiomart->helical)\n",
            "  Downloading cattrs-25.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-cache->pybiomart->helical) (4.5.1)\n",
            "Collecting url-normalize>=1.4 (from requests-cache->pybiomart->helical)\n",
            "  Downloading url_normalize-2.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->helical) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->helical) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->helical) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->helical) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->helical) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->helical) (1.22.0)\n",
            "Downloading helical-1.5.3-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.7/386.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scib-1.1.5-1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.12.7-py3-none-any.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.2/174.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_misc-0.5.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m153.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybiomart-0.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading array_api_compat-1.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading igraph-0.11.9-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m136.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scanpy-1.11.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.5-py3-none-any.whl (10 kB)\n",
            "Downloading numba-0.63.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llvmlite-0.46.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zarr-3.1.5-py3-none-any.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading leidenalg-0.10.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy_groupies-0.11.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_cache-1.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cattrs-25.3.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading numcodecs-0.16.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m159.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading url_normalize-2.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading session_info2-0.3-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: loompy, louvain\n",
            "  Building wheel for loompy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for loompy: filename=loompy-3.0.7-py3-none-any.whl size=52018 sha256=dc2c1dd57327053fb16e1d7a2baa672c3cbcce5239e711c3379d56cb8326e0dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/92/40/966ce6c42b0309032dd2f2c6bab452a88a6be78e2d6bccb338\n",
            "  Building wheel for louvain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for louvain: filename=louvain-0.8.2-cp312-cp312-linux_x86_64.whl size=971848 sha256=760da4d7d8d102f708f85234c59d2c7e3dd8fe29c2757bf19e80a79017d57501\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/de/2b/bb7ed19d84727f9f299f20cd34c42bba9c8bef7d83d2255c86\n",
            "Successfully built loompy louvain\n",
            "Installing collected packages: texttable, nvidia-cusparselt-cu12, url-normalize, triton, session-info2, nvidia-nccl-cu12, nvidia-cudnn-cu12, numpy, llvmlite, legacy-api-wrap, igraph, donfig, deprecated, cattrs, array-api-compat, scipy, scikit-misc, requests-cache, numpy-groupies, numcodecs, numba, louvain, leidenalg, hydra-core, gitpython, zarr, torch, tokenizers, pybiomart, loompy, transformers, datasets, bitsandbytes, anndata, accelerate, scanpy, scib, helical\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "  Attempting uninstall: gitpython\n",
            "    Found existing installation: GitPython 3.1.45\n",
            "    Uninstalling GitPython-3.1.45:\n",
            "      Successfully uninstalled GitPython-3.1.45\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "access 1.1.10.post3 requires scipy>=1.14.1, but you have scipy 1.13.1 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.7.0 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.7.0 which is incompatible.\n",
            "cuml-cu12 25.10.0 requires numba<0.62.0a0,>=0.60.0, but you have numba 0.63.1 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires numba<0.62.0a0,>=0.60.0, but you have numba 0.63.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.4.0 anndata-0.12.7 array-api-compat-1.13.0 bitsandbytes-0.49.0 cattrs-25.3.0 datasets-3.6.0 deprecated-1.3.1 donfig-0.8.1.post1 gitpython-3.1.44 helical-1.5.3 hydra-core-1.3.2 igraph-0.11.9 legacy-api-wrap-1.5 leidenalg-0.10.2 llvmlite-0.46.0 loompy-3.0.7 louvain-0.8.2 numba-0.63.1 numcodecs-0.16.5 numpy-2.2.6 numpy-groupies-0.11.3 nvidia-cudnn-cu12-9.5.1.17 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 pybiomart-0.2.0 requests-cache-1.2.1 scanpy-1.11.5 scib-1.1.5 scikit-misc-0.5.2 scipy-1.13.1 session-info2-0.3 texttable-1.7.0 tokenizers-0.21.4 torch-2.7.0 transformers-4.51.3 triton-3.3.0 url-normalize-2.2.1 zarr-3.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "3af3bb97cedb405f95fe6eae3de007f9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade helical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEtt89ieknTP",
        "outputId": "e8526487-3b0a-4a9e-917d-e778a4056dd8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Collecting torch\n",
            "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch)\n",
            "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.2.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m138.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl (8.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m147.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.3.0\n",
            "    Uninstalling triton-3.3.0:\n",
            "      Successfully uninstalled triton-3.3.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.26.2\n",
            "    Uninstalling nvidia-nccl-cu12-2.26.2:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.7.0\n",
            "    Uninstalling torch-2.7.0:\n",
            "      Successfully uninstalled torch-2.7.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cu126\n",
            "    Uninstalling torchvision-0.24.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.24.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "helical 1.5.3 requires torch==2.7.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 torchvision-0.24.1 triton-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Below is boiler plate code which I have taken and modularised from the Helical repo.\n",
        "\n",
        "- The boiler plate code only consists of loading, preparing and fine-tuning of the data and the Geneformer respectively. The experiments (which follows the boiler plate code) is mine.\n",
        "\n",
        "- The code can be found here https://github.com/helicalAI/helical/blob/release/examples/notebooks/Cell-Type-Classification-Fine-Tuning.ipynb"
      ],
      "metadata": {
        "id": "219Miq2JSd1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **------------------------ BOILER PLATE CODE BEGINS HERE -------------------**"
      ],
      "metadata": {
        "id": "4DOTnEI3SXFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**<u>Imports</U>**\n",
        "- Import the following packages."
      ],
      "metadata": {
        "id": "ypsOaYkZcdP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helical.utils import get_anndata_from_hf_dataset\n",
        "from helical.models.geneformer import GeneformerConfig, GeneformerFineTuningModel\n",
        "from helical.models.scgpt import scGPTConfig, scGPTFineTuningModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from scipy.stats import ks_2samp\n",
        "import matplotlib.pyplot as plt\n",
        "import logging, warnings\n",
        "import umap\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "import pynvml\n",
        "import time\n",
        "import psutil\n",
        "from pynvml import (\n",
        "    nvmlInit,\n",
        "    nvmlShutdown,\n",
        "    nvmlDeviceGetHandleByIndex,\n",
        "    nvmlDeviceGetUtilizationRates\n",
        ")\n",
        "\n",
        "\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "885uS3kkcN9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d7a93d-ec08-4bdc-d7b7-360e3a7af822"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
            "2026-01-01 17:58:26,351 - INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
            "INFO:datasets:PyTorch version 2.9.1 available.\n",
            "2026-01-01 17:58:29,564 - INFO:datasets:PyTorch version 2.9.1 available.\n",
            "INFO:datasets:Polars version 1.31.0 available.\n",
            "2026-01-01 17:58:29,567 - INFO:datasets:Polars version 1.31.0 available.\n",
            "INFO:datasets:Duckdb version 1.3.2 available.\n",
            "2026-01-01 17:58:29,569 - INFO:datasets:Duckdb version 1.3.2 available.\n",
            "INFO:datasets:TensorFlow version 2.19.0 available.\n",
            "2026-01-01 17:58:29,571 - INFO:datasets:TensorFlow version 2.19.0 available.\n",
            "INFO:datasets:JAX version 0.7.2 available.\n",
            "2026-01-01 17:58:29,573 - INFO:datasets:JAX version 0.7.2 available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Check if we have access to a GPU."
      ],
      "metadata": {
        "id": "KmFC89b3reh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "GboCpxRdkcBU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Install datasets</u>**\n",
        "- Here we will load the data required for fine-tuning and model inference."
      ],
      "metadata": {
        "id": "7eC3jCDlMxdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "  \"\"\"\n",
        "  Loads data for fine-tuning and inference.\n",
        "\n",
        "  Args:\n",
        "      None\n",
        "\n",
        "  Returns:\n",
        "      train_dataset: Dataset\n",
        "      test_dataset: Dataset\n",
        "  \"\"\"\n",
        "\n",
        "  # Load data\n",
        "  ds = load_dataset(\"helical-ai/yolksac_human\",trust_remote_code=True, download_mode=\"reuse_cache_if_exists\")\n",
        "\n",
        "  # Split data into training and testing\n",
        "  train_dataset = get_anndata_from_hf_dataset(ds[\"train\"])\n",
        "  test_dataset = get_anndata_from_hf_dataset(ds[\"test\"])\n",
        "\n",
        "  # Return both Training and Testing dataset\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "# call load_data() to create the datasets\n",
        "train_dataset, test_dataset = load_data()"
      ],
      "metadata": {
        "id": "5AHim_J7KhAG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "8446fdb8ef4f420082bde10affa2f9f5",
            "85284b02a2d447458aef5f612b17f7b6",
            "b192bf5d563f4030904607a336ac3653",
            "8b98bebd04924eb3ad29a84adba6a66c",
            "df793411605d440ba554fc74304487dc",
            "0b5f6e5a86e94ce6b1d18a00cac7fbb5",
            "5d5b9e7aa46847ddbf6d4491762437ea",
            "0dbd066a0c3342dfad499afd906d0505",
            "7f8b9ac89ebf444da0c7d6459ebd8366",
            "443785393669410cbf834dcfe492ef13",
            "fb61ccfcbb3543aa9cb93cf6ae49e5ea",
            "fa83523168e84dfd9e20121457fff2fe",
            "67d9f9cdf7bf4a27a1e217632148750a",
            "68a6dcc9aae24a3b8d6d943d4795f8c6",
            "edba8ac4fc5f47ea9e1322aab8ac67f0",
            "dd745813bb1348db9487b5016adc30e5",
            "40fd804e49064c519f6e0942a60f79d5",
            "9a0302ee767542308338ad92df62c2bf",
            "abbb02fdc0e0475190101989443b2356",
            "01c71441dc554f92bb8df4113316afde",
            "95441cecd33f4f018fb0a1b2846f096c",
            "743a890b7dd845d3a623c4bc26695c89",
            "234e3040931d4b4aa766496aac47612b",
            "2a6f2e5eefe44ef09ee506b87c123c57",
            "738490a45bed4c61ac8f9704db4d8a3a",
            "e7a966cc27524c768da2cd18947250b3",
            "17ca5fa83ac142ab8f8e2b4c436729e5",
            "2aa3621b4bea424499eb17526f46e378",
            "2478c9a3a23940a88662e499d1cac968",
            "67775ca4db434d109371da83101efa55",
            "cc39b9dc4c884434a63a826f6524645c",
            "3f599166671f40e4b04a0a38e47ee472",
            "fb329a486ab4422aa3f0f3477c1898f9",
            "219dcfb840ef4b91ab3176df102b4a49",
            "f839394c10454990b146d77c8525ab60",
            "4164d7eb115f4e0ca935d58e1af69d06",
            "59b5082b1e434c6591460d7519f2fa26",
            "9549048c665948d6ad26518a8cd85ee3",
            "1604291be4124740a286e5bafe3923dd",
            "ae9c8a084fb74baca920eab4c872162b",
            "8ad14e542e27445484da63c9bb936b6c",
            "826ce1c9896444469527eabdfec748de",
            "05cd9140a4c14a9e83977f31bd4e34d4",
            "f5415ca3424d4ff2a1adae01f891b3a0",
            "b1e73efb8cb14d048929a7105a5c2253",
            "83116df8400548f19302e1cec3a5960a",
            "f0f600b6bd464c59873bf3af7c69f7d4",
            "a3442e98f73b46a99ff86a4e6e9788bf",
            "f1f39b53f2e34b309ddcfbdf4431883c",
            "df9b225cfa224035b52020c4e6b10cf6",
            "c2dc53760ae543d5b3c2a2e8098bf31f",
            "45bdcd35181f40589ede5b4336dfdb90",
            "c9c56e7d76b2485c89138179512d3856",
            "4b9f9e47565e4d5a868eb0b99f845feb",
            "acbb653117e94dd3953160587ef40ff3"
          ]
        },
        "outputId": "de99a18c-bcc1-4021-fb8d-9366ae090bf3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8446fdb8ef4f420082bde10affa2f9f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "yolksac_human.py: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa83523168e84dfd9e20121457fff2fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "./data/17_04_24_YolkSacRaw_F158_WE_annot(…):   0%|          | 0.00/553M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "234e3040931d4b4aa766496aac47612b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "219dcfb840ef4b91ab3176df102b4a49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1e73efb8cb14d048929a7105a5c2253"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<U>Prepare training labels</U>**\n",
        "- Here we will prepare the data for fine-tuning and model inference. Here we will focus on converting string labels into unique integer classes for training."
      ],
      "metadata": {
        "id": "w3RcJ0EbDAiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_data():\n",
        "  \"\"\"\n",
        "  Cleans the data before fine tuning and inference\n",
        "\n",
        "  Args:\n",
        "      None\n",
        "\n",
        "  Returns:\n",
        "      cell_types_train: Dataset\n",
        "      cell_types_test: Dataset\n",
        "  \"\"\"\n",
        "  cell_types_train = list(np.array(train_dataset.obs[\"LVL1\"].tolist()))\n",
        "  cell_types_test = list(np.array(test_dataset.obs[\"LVL1\"].tolist()))\n",
        "\n",
        "  # We convert these string labels into unique integer classes for training\n",
        "  label_set = set(cell_types_train) | set(cell_types_test)\n",
        "  class_id_dict = dict(zip(label_set, [i for i in range(len(label_set))]))\n",
        "  id_class_dict = {v: k for k, v in class_id_dict.items()}\n",
        "\n",
        "  for i in range(len(cell_types_train)):\n",
        "      cell_types_train[i] = class_id_dict[cell_types_train[i]]\n",
        "\n",
        "  for i in range(len(cell_types_test)):\n",
        "      cell_types_test[i] = class_id_dict[cell_types_test[i]]\n",
        "\n",
        "  # Return both training and testing data\n",
        "  return cell_types_train, cell_types_test, label_set\n",
        "\n",
        "# call prep_data()\n",
        "cell_types_train, cell_types_test, label_set = prep_data()"
      ],
      "metadata": {
        "id": "RPo8_6DkKmjg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u> Fine-tuning </u>**\n",
        "- Here we will focus on fine-tuning the Geneformer. The fine_tune() function below does just that for us!"
      ],
      "metadata": {
        "id": "5IClaCOhDdVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune(train_dataset, test_dataset):\n",
        "  \"\"\"\n",
        "  fine_tune our genformer model\n",
        "\n",
        "  Args:\n",
        "      train_dataset: Dataset\n",
        "      test_dataset: Dataset\n",
        "\n",
        "  Returns:\n",
        "      geneformer_fine_tune: GeneformerFineTuningModel\n",
        "  \"\"\"\n",
        "\n",
        "  geneformer_config = GeneformerConfig(device=device, batch_size=10, model_name=\"gf-6L-10M-i2048\")\n",
        "  geneformer_fine_tune = GeneformerFineTuningModel(geneformer_config=geneformer_config, fine_tuning_head=\"classification\", output_size=len(label_set))\n",
        "\n",
        "  # Process the data so it is in the correct form for Geneformer.\n",
        "  geneformer_train_dataset = geneformer_fine_tune.process_data(train_dataset)\n",
        "  geneformer_test_dataset = geneformer_fine_tune.process_data(test_dataset)\n",
        "\n",
        "  # Geneformer makes use of the Hugging Face dataset class and so we need to add the labels as a column to this dataset.\n",
        "  geneformer_train_dataset = geneformer_train_dataset.add_column(\"LVL1\", cell_types_train)\n",
        "  geneformer_test_dataset = geneformer_test_dataset.add_column(\"LVL1\", cell_types_test)\n",
        "\n",
        "  # Fine-tune the model.\n",
        "  geneformer_fine_tune.train(train_dataset=geneformer_train_dataset.shuffle(),\n",
        "                             validation_dataset=geneformer_test_dataset, label=\"LVL1\",\n",
        "                             freeze_layers=0, epochs=1, optimizer_params={\"lr\": 1e-4},\n",
        "                             lr_scheduler_params={\"name\":\"linear\", \"num_warmup_steps\":0,\n",
        "                                                  'num_training_steps':1})\n",
        "\n",
        "  # return the fine-tuned Geneformer and the test dataset\n",
        "  return geneformer_fine_tune, geneformer_test_dataset\n",
        "\n",
        "# Call the fine_tune() function\n",
        "geneformer_fine_tune, geneformer_test_dataset = fine_tune(train_dataset, test_dataset)"
      ],
      "metadata": {
        "id": "gp96aQr6Nda_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2273f5f5-f6bb-4af8-c8ca-f4ec8a302816"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "gene_median_dictionary.pkl: 100%|██████████| 941k/941k [00:00<00:00, 1.96MB/s]\n",
            "token_dictionary.pkl: 100%|██████████| 788k/788k [00:00<00:00, 8.30MB/s]\n",
            "ensembl_mapping_dict.pkl: 100%|██████████| 3.96M/3.96M [00:00<00:00, 38.7MB/s]\n",
            "config.json: 100%|██████████| 565/565 [00:00<00:00, 1.66MB/s]\n",
            "training_args.bin: 100%|██████████| 2.61k/2.61k [00:00<00:00, 11.2MB/s]\n",
            "pytorch_model.bin: 100%|██████████| 41.2M/41.2M [00:01<00:00, 38.0MB/s]\n",
            "hsapiens_pybiomart.csv: 100%|██████████| 2.30M/2.30M [00:00<00:00, 4.01MB/s]\n",
            "Fine-Tuning: epoch 1/1: 100%|██████████| 2535/2535 [04:58<00:00,  8.51it/s, loss=0.0658]\n",
            "Fine-Tuning Validation: 100%|██████████| 634/634 [00:32<00:00, 19.46it/s, val_loss=0.0346]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **----------------------- BOILER PLATE CODE ENDS HERE ---------------------------------**"
      ],
      "metadata": {
        "id": "Wv84EucjRkv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Experiment 1: Vanilla Inference</u>**\n",
        "- Here we will simply run a baseline inference on a set of perturbations.\n",
        "- Record runtime, GPU/CPU utilization, and memory use (we will also do this for the following experiments aswell).\n",
        "- Save a small sample of model outputs (latent vectors) for all our experiments for comparison."
      ],
      "metadata": {
        "id": "_xuDb5hdhcf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Vanilla Inference (FP32)\n",
        "\n",
        "def exp1(geneformer_fine_tune, geneformer_test_dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    Vanilla Geneformer inference - no optimization. Here\n",
        "    we run geneformer_fine_tune.get_outputs(geneformer_test_dataset)\n",
        "    to obtain the predictions along with the metrics.\n",
        "\n",
        "    Args:\n",
        "      geneformer_fine_tune: GeneformerFineTuningModel\n",
        "      geneformer_test_dataset: Dataset\n",
        "\n",
        "    Returns:\n",
        "      y_pred: ndarray\n",
        "      metrics: dict\n",
        "    \"\"\"\n",
        "\n",
        "    process = psutil.Process()\n",
        "\n",
        "    # Warm up CPU stats\n",
        "    process.cpu_percent(interval=None)\n",
        "    process.cpu_percent(interval=None)\n",
        "\n",
        "    time_sum = 0\n",
        "    peak_mem = 0\n",
        "\n",
        "    geneformer_fine_tune.model.eval()\n",
        "    geneformer_fine_tune.model.to(\"cuda\")\n",
        "\n",
        "    # Reset GPU peak memory stats\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        t0 = time.time()\n",
        "        outputs = geneformer_fine_tune.get_outputs(geneformer_test_dataset)\n",
        "        torch.cuda.synchronize()\n",
        "        time_sum += time.time() - t0\n",
        "\n",
        "    # GPU peak memory\n",
        "    peak_mem = torch.cuda.max_memory_allocated()\n",
        "    peak_mem_mb = peak_mem / (1024 ** 2)\n",
        "\n",
        "    cpu_usage = process.cpu_percent(interval=None)\n",
        "\n",
        "    #  GPU utilization\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    util = nvmlDeviceGetUtilizationRates(handle)\n",
        "    gpu_util = util.gpu\n",
        "    gpu_mem_util = util.memory\n",
        "    nvmlShutdown()\n",
        "\n",
        "\n",
        "    print(classification_report(cell_types_test, outputs.argmax(axis=1)))\n",
        "    print(f\"CPU usage: {cpu_usage:.2f}%\")\n",
        "    print(f\"GPU usage: {gpu_util:.2f}%\")\n",
        "    print(f\"GPU memory usage: {gpu_mem_util:.2f}%\")\n",
        "    print(f\"Peak GPU memory: {peak_mem_mb:.2f} MB\")\n",
        "    print(f\"Avg inference time: {time_sum / len(geneformer_test_dataset):.6f}s\")\n",
        "\n",
        "    metrics = {\"CPU usage (%)\": cpu_usage,\n",
        "               \"GPU usage(%)\" : gpu_util,\n",
        "               \"GPU memory usage (%)\": gpu_mem_util,\n",
        "               \"Peak GPU memory (MB)\": peak_mem_mb,\n",
        "               \"Avg inference time (ms)\":  (time_sum * 1000/ len(geneformer_test_dataset))\n",
        "               }\n",
        "\n",
        "    return outputs.argmax(axis=1), metrics\n",
        "\n",
        "\n",
        "y_pred_exp1, metrics_exp1 = exp1(geneformer_fine_tune, geneformer_test_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY11GuzXHUPE",
        "outputId": "a37bb0e9-e2f0-4e45-8d34-4ed91ceaadb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 634/634 [00:30<00:00, 20.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 85.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 2243.59 MB\n",
            "Avg inference time: 0.004832s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Experiment 2: FP16 (Soft Quantization)</u>**\n",
        "\n",
        "- Here we will convert the precision the model's weights from FP32 to FP16.\n",
        "- By doing this, we suspect inference to be quicker and memory usage to drop.\n",
        "- Let's investigate and see if this occurs...   "
      ],
      "metadata": {
        "id": "PwdpCHQ_sYB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) FP16 (soft quantization)\n",
        "\n",
        "def exp2(geneformer_fine_tune, geneformer_test_dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    Geneformer inference using FP16() casting (soft quantization). Here\n",
        "    we run geneformer_fine_tune.get_outputs(geneformer_test_dataset)\n",
        "    to obtain the predictions along with the metrics.\n",
        "\n",
        "    Args:\n",
        "      geneformer_fine_tune: GeneformerFineTuningModel\n",
        "      geneformer_test_dataset: Dataset\n",
        "\n",
        "    Returns:\n",
        "      y_pred: ndarray\n",
        "      metrics: dict\n",
        "    \"\"\"\n",
        "\n",
        "    process = psutil.Process()\n",
        "\n",
        "    # Warm up CPU stats\n",
        "    process.cpu_percent(interval=None)\n",
        "    process.cpu_percent(interval=None)\n",
        "\n",
        "    time_sum = 0\n",
        "    peak_mem = 0\n",
        "\n",
        "    geneformer_fine_tune.model.eval()\n",
        "    # Here we implement FP16\n",
        "    geneformer_fine_tune.model.half().to(\"cuda\")\n",
        "\n",
        "    # Reset GPU peak memory stats\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        t0 = time.time()\n",
        "        outputs = geneformer_fine_tune.get_outputs(geneformer_test_dataset)\n",
        "        torch.cuda.synchronize()\n",
        "        time_sum += time.time() - t0\n",
        "\n",
        "    # GPU peak memory\n",
        "    peak_mem = torch.cuda.max_memory_allocated()\n",
        "    peak_mem_mb = peak_mem / (1024 ** 2)\n",
        "\n",
        "    cpu_usage = process.cpu_percent(interval=None)\n",
        "\n",
        "    # GPU utilization\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    util = nvmlDeviceGetUtilizationRates(handle)\n",
        "    gpu_util = util.gpu\n",
        "    gpu_mem_util = util.memory\n",
        "    nvmlShutdown()\n",
        "\n",
        "    print(classification_report(cell_types_test, outputs.argmax(axis=1)))\n",
        "    print(f\"CPU usage: {cpu_usage:.2f}%\")\n",
        "    print(f\"GPU usage: {gpu_util:.2f}%\")\n",
        "    print(f\"GPU memory usage: {gpu_mem_util:.2f}%\")\n",
        "    print(f\"Peak GPU memory: {peak_mem_mb:.2f} MB\")\n",
        "    print(f\"Avg inference time: {time_sum / len(geneformer_test_dataset):.6f}s\")\n",
        "\n",
        "    metrics = {\"CPU usage (%)\": cpu_usage,\n",
        "               \"GPU usage(%)\" : gpu_util,\n",
        "               \"GPU memory usage (%)\": gpu_mem_util,\n",
        "               \"Peak GPU memory (MB)\": peak_mem_mb,\n",
        "               \"Avg inference time (ms)\":  (time_sum * 1000/ len(geneformer_test_dataset))\n",
        "               }\n",
        "\n",
        "    return outputs.argmax(axis=1), metrics\n",
        "\n",
        "\n",
        "y_pred_exp2, metrics_exp2 = exp2(geneformer_fine_tune, geneformer_test_dataset)"
      ],
      "metadata": {
        "id": "gutTwuD0Hdpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44fd9912-de47-497a-b294-9c574df10ff8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 634/634 [00:11<00:00, 54.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 59.00%\n",
            "GPU memory usage: 19.00%\n",
            "Peak GPU memory: 1150.76 MB\n",
            "Avg inference time: 0.001821s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Experiment 3: Mixed Precision</u>**\n",
        "\n",
        "- Automatic Mixed Precision (AMP) is an optimization technique that speeds up training by using lower precision (FP16/bfloat16) where it’s safe, while keeping FP32 where it’s needed for numerical stability.\n",
        "- As a result, we should expect to see faster training and lower memory usage whilst retaining accuracy as much as possible.\n",
        "- Let's investigate and see if this occurs...   "
      ],
      "metadata": {
        "id": "s8kNoKui4TqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Mixed Precision\n",
        "\n",
        "def exp3(geneformer_fine_tune, geneformer_test_dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    Geneformer inference using Automatic Mixed Precision (AMP). Here\n",
        "    we run geneformer_fine_tune.get_outputs(geneformer_test_dataset)\n",
        "    to obtain the predictions along with the metrics.\n",
        "\n",
        "    Args:\n",
        "      geneformer_fine_tune: GeneformerFineTuningModel\n",
        "      geneformer_test_dataset: Dataset\n",
        "\n",
        "    Returns:\n",
        "      y_pred: ndarray\n",
        "      metrics: dict\n",
        "    \"\"\"\n",
        "\n",
        "    process = psutil.Process()\n",
        "\n",
        "    # Warm up CPU stats\n",
        "    process.cpu_percent(interval=None)\n",
        "    process.cpu_percent(interval=None)\n",
        "\n",
        "    time_sum = 0\n",
        "    peak_mem = 0\n",
        "\n",
        "    geneformer_fine_tune.model.eval()\n",
        "    geneformer_fine_tune.model.to(\"cuda\")\n",
        "\n",
        "    # Reset GPU peak memory stats\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Here we implement Automatic Mixed Precision (AMP)\n",
        "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "            t0 = time.time()\n",
        "            outputs = geneformer_fine_tune.get_outputs(geneformer_test_dataset)\n",
        "            torch.cuda.synchronize()\n",
        "            time_sum += time.time() - t0\n",
        "\n",
        "    # GPU peak memory\n",
        "    peak_mem = torch.cuda.max_memory_allocated()\n",
        "    peak_mem_mb = peak_mem / (1024 ** 2)\n",
        "\n",
        "    cpu_usage = process.cpu_percent(interval=None)\n",
        "\n",
        "    # GPU utilization\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    util = nvmlDeviceGetUtilizationRates(handle)\n",
        "    gpu_util = util.gpu\n",
        "    gpu_mem_util = util.memory\n",
        "    nvmlShutdown()\n",
        "\n",
        "    print(classification_report(cell_types_test, outputs.argmax(axis=1)))\n",
        "    print(f\"CPU usage: {cpu_usage:.2f}%\")\n",
        "    print(f\"GPU usage: {gpu_util:.2f}%\")\n",
        "    print(f\"GPU memory usage: {gpu_mem_util:.2f}%\")\n",
        "    print(f\"Peak GPU memory: {peak_mem_mb:.2f} MB\")\n",
        "    print(f\"Avg inference time: {time_sum / len(geneformer_test_dataset):.6f}s\")\n",
        "\n",
        "    metrics = {\"CPU usage (%)\": cpu_usage,\n",
        "               \"GPU usage(%)\" : gpu_util,\n",
        "               \"GPU memory usage (%)\": gpu_mem_util,\n",
        "               \"Peak GPU memory (MB)\": peak_mem_mb,\n",
        "               \"Avg inference time (ms)\":  (time_sum * 1000/ len(geneformer_test_dataset))\n",
        "               }\n",
        "\n",
        "    return outputs.argmax(axis=1), metrics\n",
        "\n",
        "\n",
        "y_pred_exp3, metrics_exp3 = exp3(geneformer_fine_tune, geneformer_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvxavlXP2Qmw",
        "outputId": "9d6e3838-5245-41e0-f4c8-eff6ce45b9f8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 634/634 [00:13<00:00, 48.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 64.00%\n",
            "GPU memory usage: 30.00%\n",
            "Peak GPU memory: 1230.77 MB\n",
            "Avg inference time: 0.002074s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Experiment 4: Batching </u>**\n",
        "- We will also implement batching alongside distributed inference in this experiment. This will invlolve processing multiple inputs simultaneously instead of one-at-a-time to maximise parallel computation on GPU cores and improve memory bandwidth utilization.\n",
        "- Let's investigate and see if this occurs...    \n"
      ],
      "metadata": {
        "id": "oQ5U6gj-oF-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exp4(geneformer_fine_tune, geneformer_test_dataset, max_num=100):\n",
        "\n",
        "    \"\"\"\n",
        "    Geneformer inference where we optimize the batch-size. Here we first deduce\n",
        "    the best possible batch size and then run\n",
        "    geneformer_fine_tune.get_outputs(geneformer_test_dataset)\n",
        "    to obtain the predictions along with the metrics.\n",
        "\n",
        "    Args:\n",
        "      geneformer_fine_tune: GeneformerFineTuningModel\n",
        "      geneformer_test_dataset: Dataset\n",
        "\n",
        "    Returns:\n",
        "      y_pred: ndarray\n",
        "      metrics: dict\n",
        "    \"\"\"\n",
        "\n",
        "    best_batch_num = 10\n",
        "    best_runtime = float('inf')\n",
        "    for i in range(10,max_num+1):\n",
        "        geneformer_fine_tune.config[\"batch_size\"] = i\n",
        "        y_pred, metrics = exp1(geneformer_fine_tune, geneformer_test_dataset)\n",
        "        if metrics[\"Avg inference time (ms)\"] < best_batch_num:\n",
        "          best_runtime = metrics[\"Avg inference time (ms)\"]\n",
        "          best_batch_num = i\n",
        "    print(f'The optimal batch size is {best_batch_num}. Here are the benchmarks associated with this batch size...')\n",
        "    geneformer_fine_tune.config[\"batch_size\"] = best_batch_num\n",
        "    y_pred, metrics = exp1(geneformer_fine_tune, geneformer_test_dataset)\n",
        "    return y_pred, metrics\n",
        "y_pred_exp4, metrics_exp4 = exp4(geneformer_fine_tune, geneformer_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XvCyDpE3gY9",
        "outputId": "f5aa3a64-d852-4665-fc96-3a5130aaa752"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 634/634 [00:11<00:00, 55.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 59.00%\n",
            "GPU memory usage: 19.00%\n",
            "Peak GPU memory: 1150.76 MB\n",
            "Avg inference time: 0.001809s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 576/576 [00:11<00:00, 49.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 60.00%\n",
            "GPU memory usage: 19.00%\n",
            "Peak GPU memory: 1262.09 MB\n",
            "Avg inference time: 0.001828s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 528/528 [00:11<00:00, 46.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 59.00%\n",
            "GPU memory usage: 19.00%\n",
            "Peak GPU memory: 1373.42 MB\n",
            "Avg inference time: 0.001789s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 488/488 [00:11<00:00, 43.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 60.00%\n",
            "GPU memory usage: 20.00%\n",
            "Peak GPU memory: 1484.75 MB\n",
            "Avg inference time: 0.001767s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 453/453 [00:11<00:00, 40.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 63.00%\n",
            "GPU memory usage: 21.00%\n",
            "Peak GPU memory: 1596.09 MB\n",
            "Avg inference time: 0.001778s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 423/423 [00:11<00:00, 38.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 60.00%\n",
            "GPU memory usage: 21.00%\n",
            "Peak GPU memory: 1707.43 MB\n",
            "Avg inference time: 0.001753s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 396/396 [00:10<00:00, 36.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 59.00%\n",
            "GPU memory usage: 21.00%\n",
            "Peak GPU memory: 1818.77 MB\n",
            "Avg inference time: 0.001733s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 373/373 [00:11<00:00, 33.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 62.00%\n",
            "GPU memory usage: 22.00%\n",
            "Peak GPU memory: 1930.11 MB\n",
            "Avg inference time: 0.001757s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 352/352 [00:10<00:00, 32.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 61.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 2041.46 MB\n",
            "Avg inference time: 0.001738s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 334/334 [00:10<00:00, 30.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 63.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 2152.80 MB\n",
            "Avg inference time: 0.001728s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 317/317 [00:10<00:00, 29.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 62.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 2264.15 MB\n",
            "Avg inference time: 0.001711s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 302/302 [00:10<00:00, 27.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 62.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 2375.49 MB\n",
            "Avg inference time: 0.001725s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 288/288 [00:10<00:00, 26.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 59.00%\n",
            "GPU memory usage: 22.00%\n",
            "Peak GPU memory: 2486.98 MB\n",
            "Avg inference time: 0.001718s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 276/276 [00:10<00:00, 25.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 61.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 2598.32 MB\n",
            "Avg inference time: 0.001703s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 264/264 [00:10<00:00, 24.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 61.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 2709.67 MB\n",
            "Avg inference time: 0.001713s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 254/254 [00:10<00:00, 23.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 65.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 2821.01 MB\n",
            "Avg inference time: 0.001696s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 244/244 [00:10<00:00, 22.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 64.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 2932.35 MB\n",
            "Avg inference time: 0.001695s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 235/235 [00:10<00:00, 22.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 59.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 3043.70 MB\n",
            "Avg inference time: 0.001683s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 227/227 [00:10<00:00, 21.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 63.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 3151.54 MB\n",
            "Avg inference time: 0.001688s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 219/219 [00:10<00:00, 20.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 61.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 3259.14 MB\n",
            "Avg inference time: 0.001695s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 212/212 [00:10<00:00, 19.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 62.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 3366.49 MB\n",
            "Avg inference time: 0.001685s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 205/205 [00:10<00:00, 19.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 64.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 3473.59 MB\n",
            "Avg inference time: 0.001682s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 198/198 [00:10<00:00, 18.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 66.00%\n",
            "GPU memory usage: 26.00%\n",
            "Peak GPU memory: 3580.43 MB\n",
            "Avg inference time: 0.001676s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 192/192 [00:10<00:00, 18.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 59.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 3691.16 MB\n",
            "Avg inference time: 0.001675s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 187/187 [00:10<00:00, 17.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 65.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 3797.63 MB\n",
            "Avg inference time: 0.001681s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 182/182 [00:10<00:00, 17.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 67.00%\n",
            "GPU memory usage: 26.00%\n",
            "Peak GPU memory: 3903.85 MB\n",
            "Avg inference time: 0.001677s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 176/176 [00:10<00:00, 16.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 66.00%\n",
            "GPU memory usage: 26.00%\n",
            "Peak GPU memory: 4009.83 MB\n",
            "Avg inference time: 0.001677s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 172/172 [00:10<00:00, 16.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 62.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 4120.18 MB\n",
            "Avg inference time: 0.001666s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 167/167 [00:10<00:00, 15.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 64.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 4225.78 MB\n",
            "Avg inference time: 0.001670s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 163/163 [00:10<00:00, 15.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 63.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 4336.00 MB\n",
            "Avg inference time: 0.001667s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 159/159 [00:10<00:00, 15.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 63.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 4441.23 MB\n",
            "Avg inference time: 0.001664s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 155/155 [00:10<00:00, 14.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 64.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 4551.33 MB\n",
            "Avg inference time: 0.001668s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 151/151 [00:10<00:00, 14.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 63.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 4656.18 MB\n",
            "Avg inference time: 0.001658s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 148/148 [00:10<00:00, 14.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 60.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 4766.22 MB\n",
            "Avg inference time: 0.001662s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 144/144 [00:10<00:00, 13.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 58.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 4870.70 MB\n",
            "Avg inference time: 0.001664s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 141/141 [00:10<00:00, 13.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 61.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 4980.55 MB\n",
            "Avg inference time: 0.001654s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 138/138 [00:10<00:00, 13.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 70.00%\n",
            "GPU memory usage: 28.00%\n",
            "Peak GPU memory: 5084.65 MB\n",
            "Avg inference time: 0.001656s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 135/135 [00:10<00:00, 12.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 57.00%\n",
            "GPU memory usage: 22.00%\n",
            "Peak GPU memory: 5194.37 MB\n",
            "Avg inference time: 0.001656s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 132/132 [00:10<00:00, 12.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 70.00%\n",
            "GPU memory usage: 27.00%\n",
            "Peak GPU memory: 5298.10 MB\n",
            "Avg inference time: 0.001662s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 130/130 [00:10<00:00, 12.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 61.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 5407.70 MB\n",
            "Avg inference time: 0.001660s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 127/127 [00:10<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 70.00%\n",
            "GPU memory usage: 27.00%\n",
            "Peak GPU memory: 5517.30 MB\n",
            "Avg inference time: 0.001652s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 125/125 [00:10<00:00, 11.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 67.00%\n",
            "GPU memory usage: 26.00%\n",
            "Peak GPU memory: 5620.52 MB\n",
            "Avg inference time: 0.001650s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 122/122 [00:10<00:00, 11.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 58.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 5730.00 MB\n",
            "Avg inference time: 0.001649s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 120/120 [00:10<00:00, 11.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 68.00%\n",
            "GPU memory usage: 26.00%\n",
            "Peak GPU memory: 5839.47 MB\n",
            "Avg inference time: 0.001651s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 118/118 [00:10<00:00, 11.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 67.00%\n",
            "GPU memory usage: 26.00%\n",
            "Peak GPU memory: 5942.20 MB\n",
            "Avg inference time: 0.001646s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 116/116 [00:10<00:00, 11.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 57.00%\n",
            "GPU memory usage: 22.00%\n",
            "Peak GPU memory: 6051.55 MB\n",
            "Avg inference time: 0.001650s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 114/114 [00:10<00:00, 10.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 65.00%\n",
            "GPU memory usage: 26.00%\n",
            "Peak GPU memory: 6160.90 MB\n",
            "Avg inference time: 0.001644s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 112/112 [00:10<00:00, 10.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 65.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 6270.25 MB\n",
            "Avg inference time: 0.001643s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 110/110 [00:10<00:00, 10.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 61.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 6372.35 MB\n",
            "Avg inference time: 0.001644s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 108/108 [00:10<00:00, 10.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 64.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 6481.58 MB\n",
            "Avg inference time: 0.001640s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 106/106 [00:10<00:00, 10.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 61.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 6590.81 MB\n",
            "Avg inference time: 0.001643s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 104/104 [00:10<00:00, 10.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 63.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 6700.03 MB\n",
            "Avg inference time: 0.001644s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 103/103 [00:10<00:00,  9.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 63.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 6801.51 MB\n",
            "Avg inference time: 0.001643s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 101/101 [00:10<00:00,  9.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 64.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 6910.61 MB\n",
            "Avg inference time: 0.001639s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 99/99 [00:10<00:00,  9.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 63.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 7019.71 MB\n",
            "Avg inference time: 0.001639s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 98/98 [00:10<00:00,  9.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 60.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 7130.58 MB\n",
            "Avg inference time: 0.001641s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 96/96 [00:10<00:00,  9.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 59.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 7239.65 MB\n",
            "Avg inference time: 0.001641s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 95/95 [00:10<00:00,  9.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 59.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 7340.35 MB\n",
            "Avg inference time: 0.001640s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 94/94 [00:10<00:00,  9.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 62.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 7449.29 MB\n",
            "Avg inference time: 0.001646s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 92/92 [00:10<00:00,  8.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 70.00%\n",
            "GPU memory usage: 28.00%\n",
            "Peak GPU memory: 7558.23 MB\n",
            "Avg inference time: 0.001645s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 91/91 [00:10<00:00,  8.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 68.00%\n",
            "GPU memory usage: 26.00%\n",
            "Peak GPU memory: 7667.18 MB\n",
            "Avg inference time: 0.001635s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 90/90 [00:10<00:00,  8.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 56.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 7776.13 MB\n",
            "Avg inference time: 0.001642s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 88/88 [00:10<00:00,  8.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 56.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 7876.07 MB\n",
            "Avg inference time: 0.001641s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 87/87 [00:10<00:00,  8.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 73.00%\n",
            "GPU memory usage: 29.00%\n",
            "Peak GPU memory: 7984.89 MB\n",
            "Avg inference time: 0.001635s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 86/86 [00:10<00:00,  8.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 63.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 8093.71 MB\n",
            "Avg inference time: 0.001633s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 85/85 [00:10<00:00,  8.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 55.00%\n",
            "GPU memory usage: 22.00%\n",
            "Peak GPU memory: 8202.53 MB\n",
            "Avg inference time: 0.001638s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 84/84 [00:10<00:00,  8.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 59.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 8311.35 MB\n",
            "Avg inference time: 0.001639s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 83/83 [00:10<00:00,  8.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 69.00%\n",
            "GPU memory usage: 27.00%\n",
            "Peak GPU memory: 8420.17 MB\n",
            "Avg inference time: 0.001634s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 82/82 [00:10<00:00,  7.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 61.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 8529.00 MB\n",
            "Avg inference time: 0.001633s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 81/81 [00:10<00:00,  7.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 62.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 8627.94 MB\n",
            "Avg inference time: 0.001626s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 80/80 [00:10<00:00,  7.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 76.00%\n",
            "GPU memory usage: 29.00%\n",
            "Peak GPU memory: 8736.64 MB\n",
            "Avg inference time: 0.001623s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 79/79 [00:10<00:00,  7.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 52.00%\n",
            "GPU memory usage: 21.00%\n",
            "Peak GPU memory: 8845.33 MB\n",
            "Avg inference time: 0.001630s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 78/78 [00:10<00:00,  7.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 67.00%\n",
            "GPU memory usage: 27.00%\n",
            "Peak GPU memory: 8954.03 MB\n",
            "Avg inference time: 0.001631s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 77/77 [00:10<00:00,  7.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 54.00%\n",
            "GPU memory usage: 22.00%\n",
            "Peak GPU memory: 9062.72 MB\n",
            "Avg inference time: 0.001635s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 76/76 [00:10<00:00,  7.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 62.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 9171.42 MB\n",
            "Avg inference time: 0.001629s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 75/75 [00:10<00:00,  7.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 67.00%\n",
            "GPU memory usage: 26.00%\n",
            "Peak GPU memory: 9280.12 MB\n",
            "Avg inference time: 0.001624s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 74/74 [00:10<00:00,  7.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 60.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 9388.85 MB\n",
            "Avg inference time: 0.001622s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 73/73 [00:10<00:00,  7.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 52.00%\n",
            "GPU memory usage: 21.00%\n",
            "Peak GPU memory: 9486.67 MB\n",
            "Avg inference time: 0.001625s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 72/72 [00:10<00:00,  7.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 57.00%\n",
            "GPU memory usage: 22.00%\n",
            "Peak GPU memory: 9595.24 MB\n",
            "Avg inference time: 0.001626s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 72/72 [00:10<00:00,  6.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 73.00%\n",
            "GPU memory usage: 29.00%\n",
            "Peak GPU memory: 9703.81 MB\n",
            "Avg inference time: 0.001629s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 71/71 [00:10<00:00,  6.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 62.00%\n",
            "GPU memory usage: 24.00%\n",
            "Peak GPU memory: 9812.38 MB\n",
            "Avg inference time: 0.001623s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 70/70 [00:10<00:00,  6.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 58.00%\n",
            "GPU memory usage: 23.00%\n",
            "Peak GPU memory: 9920.95 MB\n",
            "Avg inference time: 0.001621s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 69/69 [00:10<00:00,  6.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 49.00%\n",
            "GPU memory usage: 20.00%\n",
            "Peak GPU memory: 10029.52 MB\n",
            "Avg inference time: 0.001623s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 69/69 [00:10<00:00,  6.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 53.00%\n",
            "GPU memory usage: 21.00%\n",
            "Peak GPU memory: 10138.09 MB\n",
            "Avg inference time: 0.001626s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 68/68 [00:10<00:00,  6.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.30%\n",
            "GPU usage: 49.00%\n",
            "GPU memory usage: 19.00%\n",
            "Peak GPU memory: 10246.66 MB\n",
            "Avg inference time: 0.001621s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 67/67 [00:10<00:00,  6.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 64.00%\n",
            "GPU memory usage: 25.00%\n",
            "Peak GPU memory: 10355.23 MB\n",
            "Avg inference time: 0.001627s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 66/66 [00:10<00:00,  6.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 65.00%\n",
            "GPU memory usage: 26.00%\n",
            "Peak GPU memory: 10451.81 MB\n",
            "Avg inference time: 0.001621s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 66/66 [00:10<00:00,  6.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 71.00%\n",
            "GPU memory usage: 27.00%\n",
            "Peak GPU memory: 10560.25 MB\n",
            "Avg inference time: 0.001621s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 65/65 [00:10<00:00,  6.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 70.00%\n",
            "GPU memory usage: 27.00%\n",
            "Peak GPU memory: 10669.31 MB\n",
            "Avg inference time: 0.001625s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 64/64 [00:10<00:00,  6.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 71.00%\n",
            "GPU memory usage: 27.00%\n",
            "Peak GPU memory: 10777.15 MB\n",
            "Avg inference time: 0.001625s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 64/64 [00:10<00:00,  6.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.10%\n",
            "GPU usage: 56.00%\n",
            "GPU memory usage: 22.00%\n",
            "Peak GPU memory: 10885.59 MB\n",
            "Avg inference time: 0.001619s\n",
            "The optimal batch size is 100. Here are the benchmarks associated with this batch size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Outputs: 100%|██████████| 64/64 [00:10<00:00,  6.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      3001\n",
            "           1       0.97      0.98      0.98       938\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       1.00      1.00      1.00      2321\n",
            "           4       0.36      0.47      0.41        19\n",
            "           5       0.94      0.87      0.90        38\n",
            "\n",
            "    accuracy                           0.99      6336\n",
            "   macro avg       0.71      0.72      0.71      6336\n",
            "weighted avg       0.99      0.99      0.99      6336\n",
            "\n",
            "CPU usage: 100.20%\n",
            "GPU usage: 65.00%\n",
            "GPU memory usage: 26.00%\n",
            "Peak GPU memory: 10885.59 MB\n",
            "Avg inference time: 0.001622s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<U>K-S Test</U>**\n",
        "- Now we have the outputs of all our experiments, we are now going to compare the outputs of our experiments when there is optimisation compared to our vanilla inference (experiment 1). We are going to use a K-S test to do this.\n",
        "- A Kolmogorov–Smirnov (K-S) test compares distributions by measuring the maximum distance between their cumulative distribution functions (CDFs) and flags a difference if that gap is statistically unlikely."
      ],
      "metadata": {
        "id": "3upz6l3LtSt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ks_test_two_sample(sample_1, sample_2):\n",
        "    \"\"\"\n",
        "    Perform a two-sample Kolmogorov–Smirnov test.\n",
        "\n",
        "    Args:\n",
        "        sample_1: First 1D array-like sample.\n",
        "        sample_2: Second 1D array-like sample.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (ks_statistic, p_value).\n",
        "    \"\"\"\n",
        "    statistic, p_value = ks_2samp(sample_1, sample_2)\n",
        "    return statistic, p_value\n"
      ],
      "metadata": {
        "id": "iYFuFWDqtR-Y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stat12, p_value12 = ks_2samp(y_pred_exp1, y_pred_exp2)\n",
        "stat13, p_value13 = ks_2samp(y_pred_exp1, y_pred_exp3)\n",
        "stat14, p_value14 = ks_2samp(y_pred_exp1, y_pred_exp4)\n",
        "\n",
        "print(f'Experiment 1 & experiment 2 -> k-s test: {stat12}, p-value: {p_value12}')\n",
        "print(f'Experiment 1 & experiment 3 -> k-s test: {stat13}, p-value: {p_value13}')\n",
        "print(f'Experiment 1 & experiment 4 -> k-s test: {stat14}, p-value: {p_value14}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byi-tgF8vyXI",
        "outputId": "b7ea2915-624e-42ca-d272-6f0c2d9233b8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment 1 & experiment 2 -> k-s test: 0.0, p-value: 1.0\n",
            "Experiment 1 & experiment 3 -> k-s test: 0.0, p-value: 1.0\n",
            "Experiment 1 & experiment 4 -> k-s test: 0.0, p-value: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <U>**Results**</U>"
      ],
      "metadata": {
        "id": "44gVwKonwPwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 =  pd.DataFrame([metrics_exp1])\n",
        "df1['Experiments'] = 'Experiment 1 (Vanilla)'\n",
        "\n",
        "df2 =  pd.DataFrame([metrics_exp2])\n",
        "df2['Experiments'] = 'Experiment 2 (FP16)'\n",
        "\n",
        "df3 =  pd.DataFrame([metrics_exp3])\n",
        "df3['Experiments'] = 'Experiment 3 (Mixed Precision)'\n",
        "\n",
        "df4 =  pd.DataFrame([metrics_exp4])\n",
        "df4['Experiments'] = 'Experiment 4 (Batching)'\n",
        "\n",
        "benchmarks = df_stacked = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
        "benchmarks = benchmarks.set_index('Experiments').reset_index()\n",
        "benchmarks['Accuracy'] = 0.99\n",
        "benchmarks['K-S Statistic'] = 0.0\n",
        "benchmarks['P-Value'] = 1.0\n",
        "benchmarks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "A91LVIkImyWY",
        "outputId": "e976fd91-7e87-43b0-9c9d-4626621b94f5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      Experiments  CPU usage (%)  GPU usage(%)  \\\n",
              "0          Experiment 1 (Vanilla)          100.2            85   \n",
              "1             Experiment 2 (FP16)          100.1            59   \n",
              "2  Experiment 3 (Mixed Precision)          100.3            64   \n",
              "3         Experiment 4 (Batching)          100.2            65   \n",
              "\n",
              "   GPU memory usage (%)  Peak GPU memory (MB)  Avg inference time (ms)  \\\n",
              "0                    24           2243.593750                 4.831873   \n",
              "1                    19           1150.761719                 1.821086   \n",
              "2                    30           1230.770020                 2.074472   \n",
              "3                    26          10885.590820                 1.621720   \n",
              "\n",
              "   Accuracy  K-S Statistic  P-Value  \n",
              "0      0.99            0.0      1.0  \n",
              "1      0.99            0.0      1.0  \n",
              "2      0.99            0.0      1.0  \n",
              "3      0.99            0.0      1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b019d31-0bde-4bea-97a4-58b5ad500f26\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Experiments</th>\n",
              "      <th>CPU usage (%)</th>\n",
              "      <th>GPU usage(%)</th>\n",
              "      <th>GPU memory usage (%)</th>\n",
              "      <th>Peak GPU memory (MB)</th>\n",
              "      <th>Avg inference time (ms)</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>K-S Statistic</th>\n",
              "      <th>P-Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Experiment 1 (Vanilla)</td>\n",
              "      <td>100.2</td>\n",
              "      <td>85</td>\n",
              "      <td>24</td>\n",
              "      <td>2243.593750</td>\n",
              "      <td>4.831873</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Experiment 2 (FP16)</td>\n",
              "      <td>100.1</td>\n",
              "      <td>59</td>\n",
              "      <td>19</td>\n",
              "      <td>1150.761719</td>\n",
              "      <td>1.821086</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Experiment 3 (Mixed Precision)</td>\n",
              "      <td>100.3</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>1230.770020</td>\n",
              "      <td>2.074472</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Experiment 4 (Batching)</td>\n",
              "      <td>100.2</td>\n",
              "      <td>65</td>\n",
              "      <td>26</td>\n",
              "      <td>10885.590820</td>\n",
              "      <td>1.621720</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b019d31-0bde-4bea-97a4-58b5ad500f26')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8b019d31-0bde-4bea-97a4-58b5ad500f26 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8b019d31-0bde-4bea-97a4-58b5ad500f26');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-79c9a25f-fd4d-499f-ac43-a70441250e54\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-79c9a25f-fd4d-499f-ac43-a70441250e54')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-79c9a25f-fd4d-499f-ac43-a70441250e54 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_22ef3faf-731d-4545-abcc-73854ba9a49e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('benchmarks')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_22ef3faf-731d-4545-abcc-73854ba9a49e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('benchmarks');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "benchmarks",
              "summary": "{\n  \"name\": \"benchmarks\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Experiments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Experiment 2 (FP16)\",\n          \"Experiment 4 (Batching)\",\n          \"Experiment 1 (Vanilla)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CPU usage (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08164965809277376,\n        \"min\": 100.1,\n        \"max\": 100.3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          100.2,\n          100.1,\n          100.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GPU usage(%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 59,\n        \"max\": 85,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          59,\n          65,\n          85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GPU memory usage (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 19,\n        \"max\": 30,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          19,\n          26,\n          24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Peak GPU memory (MB)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4698.342533816552,\n        \"min\": 1150.76171875,\n        \"max\": 10885.5908203125,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1150.76171875,\n          10885.5908203125,\n          2243.59375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Avg inference time (ms)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5078161288153755,\n        \"min\": 1.6217201647132333,\n        \"max\": 4.831872880458832,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.821085697773731,\n          1.6217201647132333,\n          4.831872880458832\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.99,\n        \"max\": 0.99,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.99\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"K-S Statistic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"P-Value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the K-S Statistics and the P-Values, we can deduce that the outputs generated are identical throughout and applying our optimization strategies had no impact on the performance (according to the accuracy).\n",
        "- The CPU usage ~100% in all experiments, implying get_outputs() is doing CPU-side batching, padding & dataset slicing.\n",
        "- The latency appears to be the best we optimize our batch-size, we get a 3x speed-up from our vanilla inference (experiment 1).\n",
        "-  From GPU Usage (%), we found that larger GPU utilization doesn't always mean better performance/ faster inference. Experiment 1 had the highest GPU utilization but was the slowest. Experiment 2 had the lowest GPU utilization and was ~2.6x faster than experiment 1.\n",
        "- One cause for concern appeared in the Peak GPU Memory (MB). Experiment 4 had a value of 10.9GB. This is because a larger batch size means larger padded tensors."
      ],
      "metadata": {
        "id": "a3dspv2RZaZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Discussion</U>**\n",
        "\n",
        "- The aim of this challenge was to improve the Geneformer model from the helical package to make it run more efficiently over a large amount of perturbations (i.e. inference).\n",
        "- Experiment 1 (Vanilla) was our baseline. We recorded the runtime, GPU/CPU utilization, and memory use. We then saved the model outputs for comparison.\n",
        "- We then leverged FP16 (soft quantization), mixed precision and batching to optimize for runtime, GPU/CPU utilization, and memory use. We ensured these results remained comparable to baseline outputs.\n",
        "- We reported different metrics to show that your optimisation worked, while still being correct and summarized our results in a short table.\n",
        "- Due to our limited time-frame and the amount of operational overhead involved, we were unable to explore strategies such as ONNX, TensorRT and distributed inference. One form of improvement would be to explore these stratgies."
      ],
      "metadata": {
        "id": "ZPp--snfI8xZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<U>Conclusion</U>**\n",
        "- Batching provided the largest inference speedup (~3×), outperforming both FP16 casting and AMP. GPU utilization did not correlate directly with latency, highlighting kernel efficiency and launch amortization as dominant factors. FP16 (soft quantization) primarily reduced memory footprint, while batching dominated throughput gains."
      ],
      "metadata": {
        "id": "B9QVrabxNmZH"
      }
    }
  ]
}