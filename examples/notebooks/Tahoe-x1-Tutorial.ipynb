{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Tahoe-x1 Model Tutorial\n",
    "\n",
    "Run this notebook on a colab notebook with a free GPU:\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/helicalAI/helical/blob/main/examples/notebooks/Tahoe-x1-Tutorial.ipynb)\n",
    "\n",
    "This tutorial demonstrates how to use the Tahoe-x1 foundation model for single-cell RNA-seq data. Tahoe-x1 is a transformer-based model that can extract both cell and gene embeddings from raw count data.\n",
    "\n",
    "**What you'll learn in this notebook:**\n",
    "- How to load and configure the Tahoe-x1 model\n",
    "- Processing single-cell RNA-seq data for Tahoe\n",
    "- Extracting cell embeddings\n",
    "- Extracting gene embeddings\n",
    "- Visualizing embeddings with UMAP\n",
    "- Extracting attention weights for interpretability\n",
    "\n",
    "For more examples, check out our [GitHub](https://github.com/helicalAI/helical) and [documentation](https://helical.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install or update Helical to get access to the Tahoe model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-helical",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install helical --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configure logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check device availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## Load Example Dataset\n",
    "\n",
    "We'll use the human fetal yolk sac scRNA-seq dataset from Helical's Hugging Face repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\n",
    "    \"helical-ai/yolksac_human\", \n",
    "    split=\"train[:10%]\", \n",
    "    trust_remote_code=True, \n",
    "    download_mode=\"reuse_cache_if_exists\"\n",
    ")\n",
    "\n",
    "# Store labels for visualization later\n",
    "labels = dataset[\"LVL1\"]\n",
    "\n",
    "print(f\"Loaded {len(dataset)} cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convert-anndata",
   "metadata": {},
   "source": [
    "## Convert to AnnData Format\n",
    "\n",
    "Tahoe works with AnnData objects, the standard format for single-cell data in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-anndata",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.utils import get_anndata_from_hf_dataset\n",
    "\n",
    "ann_data = get_anndata_from_hf_dataset(dataset)\n",
    "print(ann_data)\n",
    "\n",
    "# For this tutorial, let's use a subset for faster processing\n",
    "ann_data_subset = ann_data[:500]  # Use first 500 cells\n",
    "labels_subset = labels[:500]\n",
    "print(f\"\\nUsing subset: {ann_data_subset.n_obs} cells, {ann_data_subset.n_vars} genes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-setup",
   "metadata": {},
   "source": [
    "## Initialize Tahoe Model\n",
    "\n",
    "Tahoe comes in three sizes (70m, 1b, 3b). Currently, the 70m model is available. The model uses Flash Attention by default for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.models.tahoe import Tahoe, TahoeConfig\n",
    "\n",
    "# Configure the Tahoe model\n",
    "tahoe_config = TahoeConfig(\n",
    "    model_size=\"70m\",  # 12-layer transformer with 512 embedding dimensions\n",
    "    batch_size=8,      # Adjust based on your GPU memory\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Initialize the model (will download weights on first use)\n",
    "tahoe = Tahoe(configurer=tahoe_config)\n",
    "\n",
    "print(\"\\nTahoe model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process-data",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "\n",
    "Tahoe requires gene names to be mapped to Ensembl IDs. The `process_data` method handles this automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data - this will map gene symbols to Ensembl IDs\n",
    "dataloader = tahoe.process_data(\n",
    "    ann_data_subset,\n",
    "    gene_names=\"gene_name\",  # Column containing gene symbols\n",
    "    use_raw_counts=True\n",
    ")\n",
    "\n",
    "print(\"Data processed and ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-embeddings-section",
   "metadata": {},
   "source": [
    "## Extract Cell Embeddings\n",
    "\n",
    "Cell embeddings capture the transcriptional state of each cell in a dense vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-cell-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cell embeddings\n",
    "cell_embeddings = tahoe.get_embeddings(dataloader)\n",
    "\n",
    "print(f\"Cell embeddings shape: {cell_embeddings.shape}\")\n",
    "print(f\"Each cell is represented by a {cell_embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-umap",
   "metadata": {},
   "source": [
    "## Visualize Cell Embeddings with UMAP\n",
    "\n",
    "Let's visualize the cell embeddings in 2D using UMAP to see how cells cluster by cell type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "umap-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce dimensionality with UMAP\n",
    "reducer = umap.UMAP(min_dist=0.1, n_components=2, n_neighbors=15, random_state=42)\n",
    "umap_embedding = reducer.fit_transform(cell_embeddings)\n",
    "\n",
    "# Create plot dataframe\n",
    "plot_df = pd.DataFrame(umap_embedding, columns=['UMAP1', 'UMAP2'])\n",
    "plot_df['Cell Type'] = labels_subset\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=plot_df, \n",
    "    x='UMAP1', \n",
    "    y='UMAP2', \n",
    "    hue='Cell Type',\n",
    "    palette='tab10',\n",
    "    s=30,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('UMAP Visualization of Tahoe Cell Embeddings', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('UMAP 1', fontsize=12)\n",
    "plt.ylabel('UMAP 2', fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Cell Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gene-embeddings-section",
   "metadata": {},
   "source": "## Extract Gene Embeddings\n\nTahoe can also extract gene embeddings for each cell. Gene embeddings are returned as a **list of pandas Series** (one per cell), where each Series contains the embeddings for genes expressed in that specific cell:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-gene-embeddings",
   "metadata": {},
   "outputs": [],
   "source": "# Get both cell and gene embeddings\ncell_embeddings, gene_embeddings = tahoe.get_embeddings(\n    dataloader,\n    return_gene_embeddings=True\n)\n\nprint(f\"Cell embeddings shape: {cell_embeddings.shape}\")\nprint(f\"Gene embeddings: {len(gene_embeddings)} cells (list of pandas Series)\")\nprint(f\"\\nFirst cell has {len(gene_embeddings[0])} genes expressed\")\n\n# Get first gene embedding from first cell\nfirst_gene_embedding = gene_embeddings[0].iloc[0]\nprint(f\"Each gene has a {len(first_gene_embedding)}-dimensional embedding\")\n\nprint(f\"\\nExample - First 5 genes in first cell:\")\nfor gene_id, embedding in list(gene_embeddings[0].items())[:5]:\n    print(f\"  {gene_id}: shape {embedding.shape}\")"
  },
  {
   "cell_type": "markdown",
   "id": "gene-viz-section",
   "metadata": {},
   "source": "## Visualize Gene Embeddings\n\nGene embeddings are returned as a list of pandas Series (one per cell), where each Series contains gene embeddings for genes expressed in that cell. Let's aggregate and visualize them:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-genes",
   "metadata": {},
   "outputs": [],
   "source": "# Aggregate gene embeddings across all cells (average per gene)\nfrom collections import defaultdict\n\ngene_embedding_accumulator = defaultdict(lambda: {'sum': None, 'count': 0})\n\n# Accumulate embeddings for each gene across all cells\nfor cell_series in gene_embeddings:\n    for gene_id, embedding in cell_series.items():\n        if gene_embedding_accumulator[gene_id]['sum'] is None:\n            gene_embedding_accumulator[gene_id]['sum'] = embedding.copy()\n        else:\n            gene_embedding_accumulator[gene_id]['sum'] += embedding\n        gene_embedding_accumulator[gene_id]['count'] += 1\n\n# Average the embeddings\naggregated_gene_embeddings = {}\nfor gene_id, data in gene_embedding_accumulator.items():\n    aggregated_gene_embeddings[gene_id] = data['sum'] / data['count']\n\nprint(f\"Aggregated gene embeddings for {len(aggregated_gene_embeddings)} unique genes\")\n\n# Convert to numpy array for visualization\ngene_names = list(aggregated_gene_embeddings.keys())\ngene_embeddings_array = np.stack(list(aggregated_gene_embeddings.values()))\n\nprint(f\"Embedding shape: {gene_embeddings_array.shape}\")\n\n# Visualize a subset of genes with UMAP\nn_genes_to_plot = min(1000, len(gene_names))\ngene_subset_idx = np.random.choice(len(gene_names), n_genes_to_plot, replace=False)\ngene_subset = gene_embeddings_array[gene_subset_idx]\n\n# UMAP for genes\ngene_reducer = umap.UMAP(min_dist=0.1, n_components=2, n_neighbors=15, random_state=42)\ngene_umap = gene_reducer.fit_transform(gene_subset)\n\n# Plot\nplt.figure(figsize=(10, 8))\nplt.scatter(gene_umap[:, 0], gene_umap[:, 1], s=10, alpha=0.5, c='steelblue')\nplt.title(f'UMAP Visualization of {n_genes_to_plot} Gene Embeddings', fontsize=14, fontweight='bold')\nplt.xlabel('UMAP 1', fontsize=12)\nplt.ylabel('UMAP 2', fontsize=12)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nYou can access gene embeddings for a specific cell:\")\nprint(f\"Example: gene_embeddings[0]['{gene_names[0]}']  # First cell, specific gene\")"
  },
  {
   "cell_type": "markdown",
   "id": "attention-section",
   "metadata": {},
   "source": [
    "## Extract Attention Weights\n",
    "\n",
    "For interpretability, you can extract attention weights from the transformer layers. This requires using the PyTorch attention implementation instead of Flash Attention.\n",
    "\n",
    "**Note:** This is slower and uses more memory than the default Flash Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with torch attention implementation\n",
    "tahoe_config_attn = TahoeConfig(\n",
    "    model_size=\"70m\",\n",
    "    batch_size=4,  # Reduce batch size for memory efficiency\n",
    "    device=device,\n",
    "    attn_impl='torch'  # Required for attention extraction\n",
    ")\n",
    "\n",
    "tahoe_attn = Tahoe(configurer=tahoe_config_attn)\n",
    "print(\"Tahoe model with attention extraction loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-for-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a smaller subset for attention extraction\n",
    "ann_data_tiny = ann_data[:50]  # Use only 50 cells\n",
    "\n",
    "dataloader_attn = tahoe_attn.process_data(\n",
    "    ann_data_tiny,\n",
    "    gene_names=\"gene_name\",\n",
    "    use_raw_counts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention weights\n",
    "cell_embeddings_attn, attention_weights = tahoe_attn.get_embeddings(\n",
    "    dataloader_attn, \n",
    "    output_attentions=True\n",
    ")\n",
    "\n",
    "print(f\"Cell embeddings shape: {cell_embeddings_attn.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights dimensions: (n_cells, n_heads, seq_length, seq_length)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-attention-section",
   "metadata": {},
   "source": [
    "## Visualize Attention Patterns\n",
    "\n",
    "Let's visualize the attention pattern for one cell to see which genes the model pays attention to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first cell and average across attention heads\n",
    "cell_idx = 0\n",
    "cell_attention = attention_weights[cell_idx]  # Shape: (n_heads, seq_len, seq_len)\n",
    "\n",
    "# Average across heads\n",
    "avg_attention = cell_attention.mean(axis=0)  # Shape: (seq_len, seq_len)\n",
    "\n",
    "# Find actual sequence length (excluding padding)\n",
    "non_zero_mask = avg_attention.sum(axis=1) > 0\n",
    "actual_seq_len = non_zero_mask.sum()\n",
    "avg_attention_trimmed = avg_attention[:actual_seq_len, :actual_seq_len]\n",
    "\n",
    "# Plot attention heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    avg_attention_trimmed[:50, :50],  # Show first 50x50 for visibility\n",
    "    cmap='viridis',\n",
    "    square=True,\n",
    "    cbar_kws={'label': 'Attention Weight'}\n",
    ")\n",
    "plt.title(f'Attention Pattern for Cell {cell_idx} (averaged across heads)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Key Position (Gene Tokens)', fontsize=12)\n",
    "plt.ylabel('Query Position (Gene Tokens)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Showing first 50x50 positions of {actual_seq_len} total sequence length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, you learned how to:\n\n1. ✅ Load and configure the Tahoe-x1 model for single-cell RNA-seq analysis\n2. ✅ Process scRNA-seq data with automatic gene symbol to Ensembl ID mapping\n3. ✅ Extract cell embeddings that capture cellular states\n4. ✅ Extract gene embeddings per cell (list of pandas Series, one per cell)\n5. ✅ Visualize embeddings using UMAP for exploratory analysis\n6. ✅ Extract and visualize attention weights for model interpretability\n\n### Key Features\n\n- **Cell embeddings**: Dense vector representations capturing cellular transcriptional states (numpy array)\n- **Gene embeddings**: List of pandas Series, one per cell. Each Series contains gene embeddings indexed by Ensembl IDs for genes expressed in that cell\n- **Attention weights**: Interpretable attention patterns (requires `attn_impl='torch'`)\n\n### Gene Embeddings Structure\n\n```python\n# gene_embeddings is a list with length = number of cells\nlen(gene_embeddings)  # e.g., 500 cells\n\n# Each element is a pandas Series for that cell\ngene_embeddings[0]  # pandas Series with gene IDs as keys\n\n# Access specific gene in specific cell\ngene_embeddings[0]['ENSG00000123456']  # numpy array of shape (embedding_dim,)\n```\n\n### Next Steps\n\n- **Cell Type Annotation**: Use the embeddings for downstream tasks like cell type classification\n- **Gene Analysis**: Analyze gene expression patterns using per-cell gene embeddings\n- **Integration**: Combine Tahoe embeddings with other analysis tools in the scRNA-seq ecosystem\n- **Fine-tuning**: Adapt the model for specific downstream tasks (see other notebooks)\n\n### Model Information\n\n- **Model**: Tahoe-x1 by Tahoe Therapeutics\n- **Hugging Face**: [tahoebio/Tahoe-x1](https://huggingface.co/tahoebio/Tahoe-x1)\n- **Architecture**: Transformer-based foundation model for scRNA-seq\n- **Available sizes**: 70m (12 layers, 512d), 1b (24 layers, 1024d), 3b (36 layers, 1536d)\n\nFor more information and examples, visit the [Helical documentation](https://helical.readthedocs.io/)."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}