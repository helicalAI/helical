{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evo 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we are going to use Evo 2 to show some of its capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The installation to use the Evo 2 model is slightly tricky, but we have made guides on how to install it pain free in `helical/models/evo_2/README.md` using either a docker image or a conda environment\n",
    "- A few things to note before attempting to run Evo 2\n",
    "    - Evo 2 requires NVIDIA GPUs with compute capability ≥8.9\n",
    "    - It has to be run on an x86_64 system\n",
    "    - We run it on Ubuntu 22.04.5 LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you have installed everything you can continue with the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports and Installs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/matthew/miniconda3/envs/helical-env-with-evo-2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "INFO:datasets:PyTorch version 2.6.0 available.\n",
      "WARNING:helical.models.scgpt.model_dir.multiomic_model:flash_attn is not installed.\n"
     ]
    }
   ],
   "source": [
    "from helical.models.evo_2 import Evo2, Evo2Config\n",
    "import subprocess\n",
    "import torch\n",
    "# import umap\n",
    "# import umap.plot\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download a CodonBERT task dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/Sanofi-Public/CodonBERT/refs/heads/master/benchmarks/CodonBERT/data/fine-tune/mRFP_Expression.csv\"\n",
    "\n",
    "output_filename = \"mRFP_Expression.csv\"\n",
    "wget_command = [\"wget\", \"-O\", output_filename, url]\n",
    "\n",
    "try:\n",
    "    subprocess.run(wget_command, check=True)\n",
    "    print(f\"File downloaded successfully as {output_filename}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(output_filename)\n",
    "train_data = dataset[dataset[\"Split\"] == \"train\"]\n",
    "eval_data = dataset[dataset[\"Split\"] == \"val\"]\n",
    "test_data = dataset[dataset[\"Split\"] == \"test\"]\n",
    "\n",
    "train_data[\"Sequence\"] = train_data[\"Sequence\"].apply(lambda x: x.replace(\"U\", \"T\"))\n",
    "eval_data[\"Sequence\"] = eval_data[\"Sequence\"].apply(lambda x: x.replace(\"U\", \"T\"))\n",
    "test_data[\"Sequence\"] = test_data[\"Sequence\"].apply(lambda x: x.replace(\"U\", \"T\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining our Evo 2 7B model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 67.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra keys in state_dict: {'blocks.17.mixer.dense._extra_state', 'unembed.weight', 'blocks.31.mixer.dense._extra_state', 'blocks.24.mixer.dense._extra_state', 'blocks.10.mixer.dense._extra_state', 'blocks.3.mixer.dense._extra_state'}\n"
     ]
    }
   ],
   "source": [
    "evo2 = Evo2(Evo2Config(model_name=\"evo2-7b-base\", batch_size=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare our data for the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StripedHyena(\n",
       "  (embedding_layer): VocabParallelEmbedding(512, 4096)\n",
       "  (blocks): ModuleList(\n",
       "    (0): ParallelGatedConvBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (filter): HyenaCascade()\n",
       "      (projections): TELinear()\n",
       "      (out_filter_dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (l1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l3): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1-2): 2 x ParallelGatedConvBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (filter): HyenaCascade()\n",
       "      (projections): TELinear()\n",
       "      (out_filter_dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (act): Identity()\n",
       "        (l1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l3): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): AttentionBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (inner_mha_cls): MHA(\n",
       "        (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "        (inner_attn): FlashSelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): FlashCrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (rotary_emb): LinearlyScaledRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (act): Identity()\n",
       "        (l1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l3): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (4-9): 6 x ParallelGatedConvBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (filter): HyenaCascade()\n",
       "      (projections): TELinear()\n",
       "      (out_filter_dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (act): Identity()\n",
       "        (l1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l3): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (10): AttentionBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (inner_mha_cls): MHA(\n",
       "        (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "        (inner_attn): FlashSelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): FlashCrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (rotary_emb): LinearlyScaledRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (act): Identity()\n",
       "        (l1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l3): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (11-16): 6 x ParallelGatedConvBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (filter): HyenaCascade()\n",
       "      (projections): TELinear()\n",
       "      (out_filter_dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (act): Identity()\n",
       "        (l1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l3): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (17): AttentionBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (inner_mha_cls): MHA(\n",
       "        (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "        (inner_attn): FlashSelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): FlashCrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (rotary_emb): LinearlyScaledRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (act): Identity()\n",
       "        (l1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l3): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (18-23): 6 x ParallelGatedConvBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (filter): HyenaCascade()\n",
       "      (projections): TELinear()\n",
       "      (out_filter_dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (act): Identity()\n",
       "        (l1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l3): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (24): AttentionBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (inner_mha_cls): MHA(\n",
       "        (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "        (inner_attn): FlashSelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): FlashCrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (rotary_emb): LinearlyScaledRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (act): Identity()\n",
       "        (l1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l3): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (25-30): 6 x ParallelGatedConvBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (filter): HyenaCascade()\n",
       "      (projections): TELinear()\n",
       "      (out_filter_dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (act): Identity()\n",
       "        (l1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l3): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (31): AttentionBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (inner_mha_cls): MHA(\n",
       "        (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "        (inner_attn): FlashSelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): FlashCrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (rotary_emb): LinearlyScaledRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (act): Identity()\n",
       "        (l1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (l3): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (unembed): Lambda()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evo2.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = evo2.process_data(train_data)\n",
    "eval_dataset = evo2.process_data(eval_data)\n",
    "test_dataset = evo2.process_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we are going to generate embeddings for the different sequences using Evo 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 1021/1021 [04:07<00:00,  4.13it/s]\n",
      "Getting embeddings: 100%|██████████| 219/219 [00:52<00:00,  4.13it/s]\n",
      "Getting embeddings: 100%|██████████| 219/219 [00:52<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = evo2.get_embeddings(train_dataset)\n",
    "eval_embeddings = evo2.get_embeddings(eval_dataset)\n",
    "test_embeddings = evo2.get_embeddings(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved successfully\n"
     ]
    }
   ],
   "source": [
    "np.savez(\"train_embeddings.npz\", embeddings=np.array(train_embeddings[\"embeddings\"]), original_lengths=np.array(train_embeddings[\"original_lengths\"]))\n",
    "np.savez(\"eval_embeddings.npz\", embeddings=np.array(eval_embeddings[\"embeddings\"]), original_lengths=np.array(eval_embeddings[\"original_lengths\"]))\n",
    "np.savez(\"test_embeddings.npz\", embeddings=np.array(test_embeddings[\"embeddings\"]), original_lengths=np.array(test_embeddings[\"original_lengths\"]))\n",
    "print(\"Embeddings saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the embeddings from the paths so we don't have to regenerate the embeddings everytime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1021, 678, 4096), (219, 678, 4096), (219, 678, 4096))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings = np.load(\"train_embeddings.npz\")\n",
    "eval_embeddings = np.load(\"eval_embeddings.npz\")\n",
    "test_embeddings = np.load(\"test_embeddings.npz\")\n",
    "train_embeddings[\"embeddings\"].shape, eval_embeddings[\"embeddings\"].shape, test_embeddings[\"embeddings\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We define a probing MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (1): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "# Define the model\n",
    "import torch.nn as nn\n",
    "\n",
    "head_model = nn.Sequential(\n",
    "    nn.Linear(4096, 512),\n",
    "    nn.Linear(512, 1)\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize weights using Xavier initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.zeros_(m.bias)\n",
    "\n",
    "head_model.apply(init_weights)\n",
    "\n",
    "head_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "def train_model(model: torch.nn.Sequential,\n",
    "                X_train: torch.Tensor,  \n",
    "                y_train: torch.Tensor,  \n",
    "                X_val: torch.Tensor, \n",
    "                y_val: torch.Tensor, \n",
    "                optimizer = torch.optim.Adam, \n",
    "                loss_fn = torch.nn.functional.mse_loss,\n",
    "                num_epochs = 50,\n",
    "                batch = 64,\n",
    "                lr_scheduler_step=30,\n",
    "                lr_scheduler_gamma=0.1):    \n",
    "\n",
    "    # Create DataLoader for batching\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "\n",
    "    # Validation dataset\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "\n",
    "    optimizer = optimizer(model.parameters(), lr=0.0001)  # Set an initial learning rate\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_scheduler_step, gamma=lr_scheduler_gamma)\n",
    "\n",
    "    # Ensure model is in training mode\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        average_train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            average_train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {average_train_loss/len(train_loader)}\")\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation phase (optional)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for val_X, val_y in val_loader:\n",
    "                val_outputs = model(val_X)\n",
    "                val_loss = loss_fn(val_outputs, val_y)\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}, Validation Loss: {sum(val_losses)/len(val_losses)}, Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "        \n",
    "        # Set back to training mode for next epoch\n",
    "        model.train()\n",
    "        \n",
    "    model.eval()   \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 102.69164276123047\n",
      "Epoch 1, Validation Loss: 102.46187591552734, Learning Rate: 0.0001\n",
      "Epoch 2, Training Loss: 101.68602132797241\n",
      "Epoch 2, Validation Loss: 101.42153358459473, Learning Rate: 0.0001\n",
      "Epoch 3, Training Loss: 100.61723613739014\n",
      "Epoch 3, Validation Loss: 100.30001640319824, Learning Rate: 0.0001\n",
      "Epoch 4, Training Loss: 99.44958639144897\n",
      "Epoch 4, Validation Loss: 99.08520698547363, Learning Rate: 0.0001\n",
      "Epoch 5, Training Loss: 98.18503952026367\n",
      "Epoch 5, Validation Loss: 97.77401924133301, Learning Rate: 0.0001\n",
      "Epoch 6, Training Loss: 96.83944082260132\n",
      "Epoch 6, Validation Loss: 96.36771392822266, Learning Rate: 0.0001\n",
      "Epoch 7, Training Loss: 95.3786392211914\n",
      "Epoch 7, Validation Loss: 94.86300849914551, Learning Rate: 0.0001\n",
      "Epoch 8, Training Loss: 93.83050775527954\n",
      "Epoch 8, Validation Loss: 93.2645092010498, Learning Rate: 0.0001\n",
      "Epoch 9, Training Loss: 92.19836378097534\n",
      "Epoch 9, Validation Loss: 91.57369422912598, Learning Rate: 0.0001\n",
      "Epoch 10, Training Loss: 90.47625780105591\n",
      "Epoch 10, Validation Loss: 89.79326057434082, Learning Rate: 0.0001\n",
      "Epoch 11, Training Loss: 88.65392732620239\n",
      "Epoch 11, Validation Loss: 87.92904663085938, Learning Rate: 0.0001\n",
      "Epoch 12, Training Loss: 86.74922275543213\n",
      "Epoch 12, Validation Loss: 85.98402404785156, Learning Rate: 0.0001\n",
      "Epoch 13, Training Loss: 84.77851581573486\n",
      "Epoch 13, Validation Loss: 83.96563339233398, Learning Rate: 0.0001\n",
      "Epoch 14, Training Loss: 82.7172679901123\n",
      "Epoch 14, Validation Loss: 81.87395668029785, Learning Rate: 0.0001\n",
      "Epoch 15, Training Loss: 80.59885597229004\n",
      "Epoch 15, Validation Loss: 79.71937942504883, Learning Rate: 0.0001\n",
      "Epoch 16, Training Loss: 78.4215989112854\n",
      "Epoch 16, Validation Loss: 77.50562477111816, Learning Rate: 0.0001\n",
      "Epoch 17, Training Loss: 76.1914644241333\n",
      "Epoch 17, Validation Loss: 75.24242401123047, Learning Rate: 0.0001\n",
      "Epoch 18, Training Loss: 73.90734100341797\n",
      "Epoch 18, Validation Loss: 72.93191528320312, Learning Rate: 0.0001\n",
      "Epoch 19, Training Loss: 71.58275651931763\n",
      "Epoch 19, Validation Loss: 70.58097839355469, Learning Rate: 0.0001\n",
      "Epoch 20, Training Loss: 69.22451734542847\n",
      "Epoch 20, Validation Loss: 68.1972713470459, Learning Rate: 0.0001\n",
      "Epoch 21, Training Loss: 66.82825660705566\n",
      "Epoch 21, Validation Loss: 65.78763008117676, Learning Rate: 0.0001\n",
      "Epoch 22, Training Loss: 64.41117000579834\n",
      "Epoch 22, Validation Loss: 63.36110591888428, Learning Rate: 0.0001\n",
      "Epoch 23, Training Loss: 61.978378772735596\n",
      "Epoch 23, Validation Loss: 60.91402530670166, Learning Rate: 0.0001\n",
      "Epoch 24, Training Loss: 59.54652142524719\n",
      "Epoch 24, Validation Loss: 58.46961975097656, Learning Rate: 0.0001\n",
      "Epoch 25, Training Loss: 57.09770393371582\n",
      "Epoch 25, Validation Loss: 56.02560329437256, Learning Rate: 0.0001\n",
      "Epoch 26, Training Loss: 54.654845237731934\n",
      "Epoch 26, Validation Loss: 53.58471965789795, Learning Rate: 0.0001\n",
      "Epoch 27, Training Loss: 52.23707914352417\n",
      "Epoch 27, Validation Loss: 51.15685176849365, Learning Rate: 0.0001\n",
      "Epoch 28, Training Loss: 49.82606744766235\n",
      "Epoch 28, Validation Loss: 48.7519006729126, Learning Rate: 0.0001\n",
      "Epoch 29, Training Loss: 47.43284511566162\n",
      "Epoch 29, Validation Loss: 46.3744010925293, Learning Rate: 0.0001\n",
      "Epoch 30, Training Loss: 45.07570552825928\n",
      "Epoch 30, Validation Loss: 44.03001689910889, Learning Rate: 1e-05\n",
      "Epoch 31, Training Loss: 43.732484579086304\n",
      "Epoch 31, Validation Loss: 43.798240661621094, Learning Rate: 1e-05\n",
      "Epoch 32, Training Loss: 43.49456858634949\n",
      "Epoch 32, Validation Loss: 43.569101333618164, Learning Rate: 1e-05\n",
      "Epoch 33, Training Loss: 43.26696062088013\n",
      "Epoch 33, Validation Loss: 43.341965675354004, Learning Rate: 1e-05\n",
      "Epoch 34, Training Loss: 43.042640924453735\n",
      "Epoch 34, Validation Loss: 43.116713523864746, Learning Rate: 1e-05\n",
      "Epoch 35, Training Loss: 42.82119917869568\n",
      "Epoch 35, Validation Loss: 42.892958641052246, Learning Rate: 1e-05\n",
      "Epoch 36, Training Loss: 42.59666562080383\n",
      "Epoch 36, Validation Loss: 42.6705904006958, Learning Rate: 1e-05\n",
      "Epoch 37, Training Loss: 42.376012086868286\n",
      "Epoch 37, Validation Loss: 42.449889183044434, Learning Rate: 1e-05\n",
      "Epoch 38, Training Loss: 42.148736000061035\n",
      "Epoch 38, Validation Loss: 42.230743408203125, Learning Rate: 1e-05\n",
      "Epoch 39, Training Loss: 41.93769025802612\n",
      "Epoch 39, Validation Loss: 42.01285266876221, Learning Rate: 1e-05\n",
      "Epoch 40, Training Loss: 41.72186470031738\n",
      "Epoch 40, Validation Loss: 41.79573631286621, Learning Rate: 1e-05\n",
      "Epoch 41, Training Loss: 41.50007176399231\n",
      "Epoch 41, Validation Loss: 41.5798397064209, Learning Rate: 1e-05\n",
      "Epoch 42, Training Loss: 41.289618730545044\n",
      "Epoch 42, Validation Loss: 41.36503028869629, Learning Rate: 1e-05\n",
      "Epoch 43, Training Loss: 41.074676752090454\n",
      "Epoch 43, Validation Loss: 41.151185035705566, Learning Rate: 1e-05\n",
      "Epoch 44, Training Loss: 40.86162829399109\n",
      "Epoch 44, Validation Loss: 40.93846607208252, Learning Rate: 1e-05\n",
      "Epoch 45, Training Loss: 40.65360188484192\n",
      "Epoch 45, Validation Loss: 40.726508140563965, Learning Rate: 1e-05\n",
      "Epoch 46, Training Loss: 40.43774724006653\n",
      "Epoch 46, Validation Loss: 40.5152006149292, Learning Rate: 1e-05\n",
      "Epoch 47, Training Loss: 40.23125076293945\n",
      "Epoch 47, Validation Loss: 40.30503749847412, Learning Rate: 1e-05\n",
      "Epoch 48, Training Loss: 40.017547607421875\n",
      "Epoch 48, Validation Loss: 40.09541320800781, Learning Rate: 1e-05\n",
      "Epoch 49, Training Loss: 39.81489133834839\n",
      "Epoch 49, Validation Loss: 39.88708305358887, Learning Rate: 1e-05\n",
      "Epoch 50, Training Loss: 39.605488300323486\n",
      "Epoch 50, Validation Loss: 39.67900371551514, Learning Rate: 1e-05\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([train_emb[-1] for train_emb, original_len in zip(train_embeddings[\"embeddings\"], train_embeddings[\"original_lengths\"])])\n",
    "X_val = np.array([eval_emb[-1] for eval_emb, original_len in zip(eval_embeddings[\"embeddings\"], eval_embeddings[\"original_lengths\"])])\n",
    "\n",
    "y_train, y_eval = train_data[\"Value\"].to_numpy(), eval_data[\"Value\"].to_numpy()\n",
    "\n",
    "head_model_evo_2 = deepcopy(head_model)\n",
    "head_model_evo_2 = train_model(model=head_model_evo_2, \n",
    "                               X_train=torch.from_numpy(X_train), \n",
    "                               y_train=torch.tensor(y_train, dtype=torch.float32).unsqueeze(1), \n",
    "                               X_val=torch.from_numpy(X_val), \n",
    "                               y_val=torch.tensor(y_eval, dtype=torch.float32).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((219,), (219,))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([test_emb[-1] for test_emb, original_len in zip(test_embeddings[\"embeddings\"], test_embeddings[\"original_lengths\"])])\n",
    "X_test = X_test\n",
    "y_test = test_data[\"Value\"].to_numpy()\n",
    "predictions_nn = head_model_evo_2(torch.from_numpy(X_test))\n",
    "y_pred = predictions_nn.detach().cpu().numpy().squeeze()\n",
    "y_true = np.array(y_test)\n",
    "y_pred.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.27373123168945"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "loss_test = loss(predictions_nn, torch.tensor(y_test, dtype=torch.float32))\n",
    "loss_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'spearmanr': 0.017463609316120843}, {'pearsonr': 0.00015963052435425887})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "spearmanr_metric = evaluate.load(\"spearmanr\")\n",
    "pearson_metric = evaluate.load(\"pearsonr\")\n",
    "results_spearman = spearmanr_metric.compute(references=y_true, predictions=y_pred)\n",
    "result_pearson = pearson_metric.compute(references=y_true, predictions=y_pred)\n",
    "results_spearman, result_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helical-env-with-evo-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
