{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f5b3bc",
   "metadata": {},
   "source": [
    "This notebook goes over how to use `STATE` using `helical`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cbea7",
   "metadata": {},
   "source": [
    "# Download Example Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a40e1b8",
   "metadata": {},
   "source": [
    "We start by using the helical downloader to obtain an example huggingface dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed699842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.utils.downloader import Downloader\n",
    "from pathlib import Path\n",
    "\n",
    "downloader = Downloader()\n",
    "downloader.download_via_link(\n",
    "    Path(\"yolksac_human.h5ad\"),\n",
    "    \"https://huggingface.co/datasets/helical-ai/yolksac_human/resolve/main/data/17_04_24_YolkSacRaw_F158_WE_annots.h5ad?download=true\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e8491",
   "metadata": {},
   "source": [
    "# STATE Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ed3a3",
   "metadata": {},
   "source": [
    "Using the STATE model we can obtain single cell transcriptome embeddings. We first slice the dataset for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data \n",
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad(\"yolksac_human.h5ad\")\n",
    "# for demonstration we subset to 10 cells and 2000 genes\n",
    "adata = adata[:10, :2000].copy()\n",
    "\n",
    "print(adata.shape)\n",
    "n_cells = adata.n_obs\n",
    "print(n_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1c1b5",
   "metadata": {},
   "source": [
    "Initialise the model - this will download the relevant files needed in `.cache/helical/state/`. It will download the necessary files when run the first time so will take slightly longer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e73cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.models.state import stateConfig\n",
    "from helical.models.state import stateEmbed\n",
    "\n",
    "state_config = stateConfig(batch_size=16)\n",
    "state_embed = stateEmbed(configurer=state_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b4c72",
   "metadata": {},
   "source": [
    "We process the data by calling `state_embed.process_data` and pass this into `state_embed.get_embeddings` to get the final embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b1f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = state_embed.process_data(adata=adata)\n",
    "embeddings = state_embed.get_embeddings(processed_data)\n",
    "\n",
    "# note that the STATE model returns a numpy array of shape (n_cells, 1024)\n",
    "print(embeddings.shape)\n",
    "print(type(embeddings))\n",
    "\n",
    "# store the embeddings in adata.obsm['state_emb']\n",
    "adata.obsm['state_emb'] = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861511f",
   "metadata": {},
   "source": [
    "# STATE Perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e43c1bb",
   "metadata": {},
   "source": [
    "To use the perturbation model you can either pass in embeddings by specifiyng the `embed_key` arguement in `stateConfig` or use the deafult `None` value in which case the expression values are used (`adata.X`).\n",
    "\n",
    "For use of previous embeddings, the `embed_key` must exist in `adata.obsm[<embed_key>]` otherwise an error will be thrown. When set to `None` the model uses `adata.X`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6cd96",
   "metadata": {},
   "source": [
    "Let's create some dummy data for the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abafa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# some default control and non-control perturbations\n",
    "perturbations = [\n",
    "    \"[('DMSO_TF', 0.0, 'uM')]\",  # Control\n",
    "    \"[('Aspirin', 0.5, 'uM')]\",\n",
    "    \"[('Dexamethasone', 1.0, 'uM')]\",\n",
    "]\n",
    "\n",
    "n_cells = adata.n_obs\n",
    "# we assign perturbations to cells randomly\n",
    "adata.obs['target_gene'] = np.random.choice(perturbations, size=n_cells)\n",
    "adata.obs['cell_type'] = adata.obs['LVL1']  # Use your cell type column\n",
    "# we can also add a batch variable to take into account batch effects\n",
    "batch_labels = np.random.choice(['batch_1', 'batch_2', 'batch_3', 'batch_4'], size=n_cells)\n",
    "adata.obs['batch_var'] = batch_labels\n",
    "\n",
    "config = stateConfig(\n",
    "    embed_key='state_emb', # our custom embedding key from above\n",
    "    pert_col=\"target_gene\",\n",
    "    celltype_col=\"cell_type\",\n",
    "    control_pert=\"[('DMSO_TF', 0.0, 'uM')]\",\n",
    "    output_path=\"yolksac_perturbed.h5ad\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c11cfb",
   "metadata": {},
   "source": [
    "Now we can run the perturbation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab93647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.models.state import stateTransitionModel\n",
    "\n",
    "state_transition = stateTransitionModel(configurer=config)\n",
    "\n",
    "# again we process the data and get the perturbed embeddings\n",
    "processed_data = state_transition.process_data(adata)\n",
    "perturbed_embeds = state_transition.get_embeddings(processed_data)\n",
    "\n",
    "print(perturbed_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582345dd",
   "metadata": {},
   "source": [
    "# Finetuning STATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e835b83",
   "metadata": {},
   "source": [
    "We can finetune the STATE perturbation embeddings using an additional head for downstream classification and regression. Below is a dummy example using data above to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.models.state import stateFineTuningModel\n",
    "\n",
    "# Dummy cell types and labels for demonstration\n",
    "cell_types = list(adata.obs['LVL1'])\n",
    "label_set = set(cell_types)\n",
    "print(f\"Found {len(label_set)} unique cell types:\")\n",
    "\n",
    "config = stateConfig(\n",
    "    embed_key=\"state_emb\",  # Use gene expression instead of embeddings\n",
    "    pert_col=\"target_gene\",\n",
    "    celltype_col=\"cell_type\",\n",
    "    control_pert=\"[('DMSO_TF', 0.0, 'uM')]\",\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "# Create the fine-tuning model - we use a classification head for demonstration\n",
    "model = stateFineTuningModel(\n",
    "    configurer=config, \n",
    "    fine_tuning_head=\"classification\", \n",
    "    output_size=len(label_set),\n",
    ")\n",
    "\n",
    "# Process the data for training - returns a dataset object\n",
    "data = model.process_data(adata)\n",
    "\n",
    "# Create a dictionary mapping the classes to unique integers for training\n",
    "class_id_dict = dict(zip(label_set, [i for i in range(len(label_set))]))\n",
    "\n",
    "# Convert cell type labels to integers\n",
    "cell_type_labels = [class_id_dict[ct] for ct in cell_types]\n",
    "\n",
    "print(f\"Class mapping: {class_id_dict}\")\n",
    "\n",
    "# Fine-tune\n",
    "model.train(train_input_data=data, train_labels=cell_type_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53936df",
   "metadata": {},
   "source": [
    "# Training STATE for the Virtual Cell Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be814ae",
   "metadata": {},
   "source": [
    "We use data from the Virtual Cell Challenge for model training and downstream inference. For this we require the VCC dataset as in the colab notebook by the authors. See the relevant code snippet for the entire dataset in the below colab notebook:\n",
    "\n",
    "[STATE Colab Notebook](https://colab.research.google.com/drive/1QKOtYP7bMpdgDJEipDxaJqOchv7oQ-_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e7ad8",
   "metadata": {},
   "source": [
    "For demonstration we use a subset of the data. We also need to change the filepath in `starter.toml` to point to the correct dataset location (see top of file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.utils.downloader import Downloader\n",
    "import toml\n",
    "\n",
    "downloader = Downloader()\n",
    "downloader.download_via_name(\"sample_vcc_data\")\n",
    "\n",
    "# quick fix to change the filepath in starter.toml\n",
    "toml.dump({**toml.load(open(\"sample_vcc_data/starter.toml\")),**{\"datasets\": {\"replogle_h1\": str(Path(\"sample_vcc_data\").absolute() / \"{rpe1,hepg2}.h5\")}},},open(\"sample_vcc_data/starter.toml\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974174ac",
   "metadata": {},
   "source": [
    "We use the `stateTransitionTrainModel` class and initialise `trainConfigs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a186cbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.6.0 available.\n",
      "INFO:datasets:Polars version 1.33.0 available.\n"
     ]
    }
   ],
   "source": [
    "from helical.models.state import stateTransitionTrainModel\n",
    "from helical.models.state.train_configs import trainConfig\n",
    "\n",
    "# default configs for competition dataset\n",
    "train_config = trainConfig(\n",
    "    output_dir=\"sample_vcc_data\",\n",
    "    name=\"first_run\",\n",
    "    toml_config_path=\"sample_vcc_data/starter.toml\",\n",
    "    checkpoint_name=\"final.ckpt\",\n",
    "    max_steps=40000,\n",
    "    max_epochs=1,\n",
    "    ckpt_every_n_steps=20000,\n",
    "    num_workers=4,\n",
    "    batch_col=\"batch_var\",\n",
    "    pert_col=\"target_gene\",\n",
    "    cell_type_key=\"cell_type\",\n",
    "    control_pert=\"non-targeting\",\n",
    "    perturbation_features_file=\"sample_vcc_data/ESM2_pert_features.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f23307fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Seed set to 42\n",
      "WARNING:cell_load.config:Dataset path does not exist: /home/rasched/final_helical_with_state/helical/helical/models/state/competition_support_set/{rpe1,hepg2}.h5\n",
      "INFO:cell_load.config:Configuration validation passed\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:Initializing DataModule: batch_size=16, workers=4, random_seed=42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rasched/final_helical_with_state/helical/helical/models/state/competition_support_set/{rpe1,hepg2}.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cell_load.data_modules.perturbation_dataloader:Set 2 missing perturbations to zero vectors.\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:Loaded custom perturbation featurizations for 19792 perturbations.\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:Processing dataset replogle_h1:\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:  - Training dataset: True\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:  - Zeroshot cell types: ['hepg2']\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:  - Fewshot cell types: []\n",
      "Processing replogle_h1:   0%|          | 0/2 [00:00<?, ?it/s]WARNING:cell_load.dataset._perturbation:No cell barcode information found in /home/rasched/final_helical_with_state/helical/helical/models/state/competition_support_set/rpe1.h5. Generating generic barcodes.\n",
      "Processing replogle_h1:   0%|          | 0/2 [00:00<?, ?it/s]WARNING:cell_load.dataset._perturbation:No cell barcode information found in /home/rasched/final_helical_with_state/helical/helical/models/state/competition_support_set/hepg2.h5. Generating generic barcodes.\n",
      "Processing replogle_h1: 100%|██████████| 2/2 [00:00<00:00, 151.30it/s]\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:\n",
      "\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:Done! Train / Val / Test splits: 1 / 0 / 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed rpe1: 22317 train, 0 val, 0 test\n",
      "Processed hepg2: 0 train, 0 val, 9386 test\n",
      "Model created. Estimated params size: 0.61 GB and 650505936 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.state.state_train:Loggers and callbacks set up.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:helical.models.state.state_train:Starting trainer fit.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer built successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: \n",
      "  | Name                 | Type                    | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | loss_fn              | SamplesLoss             | 0      | train\n",
      "1 | pert_encoder         | Sequential              | 4.8 M  | train\n",
      "2 | basal_encoder        | Linear                  | 12.2 M | train\n",
      "3 | transformer_backbone | LlamaBidirectionalModel | 50.4 M | train\n",
      "4 | project_out          | Sequential              | 13.5 M | train\n",
      "5 | final_down_then_up   | Sequential              | 81.7 M | train\n",
      "6 | relu                 | ReLU                    | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "141 M     Trainable params\n",
      "21.5 M    Non-trainable params\n",
      "162 M     Total params\n",
      "650.506   Total estimated model params size (MB)\n",
      "86        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name                 | Type                    | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | loss_fn              | SamplesLoss             | 0      | train\n",
      "1 | pert_encoder         | Sequential              | 4.8 M  | train\n",
      "2 | basal_encoder        | Linear                  | 12.2 M | train\n",
      "3 | transformer_backbone | LlamaBidirectionalModel | 50.4 M | train\n",
      "4 | project_out          | Sequential              | 13.5 M | train\n",
      "5 | final_down_then_up   | Sequential              | 81.7 M | train\n",
      "6 | relu                 | ReLU                    | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "141 M     Trainable params\n",
      "21.5 M    Non-trainable params\n",
      "162 M     Total params\n",
      "650.506   Total estimated model params size (MB)\n",
      "86        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cell_load.data_modules.samplers:Creating perturbation batch sampler with metadata caching (using codes)...\n",
      "INFO:cell_load.data_modules.samplers:Total # cells 9386. Cell set size mean / std before resampling: 78.22 / 47.32.\n",
      "INFO:cell_load.data_modules.samplers:Creating meta-batches with cell_sentence_len=128...\n",
      "INFO:cell_load.data_modules.samplers:Of all batches, 51 were full and 69 were partial.\n",
      "INFO:cell_load.data_modules.samplers:Sampler created with 8 batches in 0.01 seconds.\n",
      "INFO:cell_load.data_modules.samplers:Of all batches, 51 were full and 69 were partial.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cell_load.data_modules.samplers:Creating perturbation batch sampler with metadata caching (using codes)...\n",
      "INFO:cell_load.data_modules.samplers:Total # cells 22317. Cell set size mean / std before resampling: 107.29 / 36.13.\n",
      "INFO:cell_load.data_modules.samplers:Creating meta-batches with cell_sentence_len=128...\n",
      "INFO:cell_load.data_modules.samplers:Of all batches, 139 were full and 69 were partial.\n",
      "INFO:cell_load.data_modules.samplers:Sampler created with 13 batches in 0.02 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cell_load.data_modules.samplers:Of all batches, 139 were full and 69 were partial.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 13/13 [00:04<00:00,  2.94it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 13/13 [00:04<00:00,  2.94it/s, v_num=0]\n",
      "Training completed, saving final checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cell_load.data_modules.samplers:Creating perturbation batch sampler with metadata caching (using codes)...\n",
      "INFO:cell_load.data_modules.samplers:Total # cells 9386. Cell set size mean / std before resampling: 78.22 / 47.32.\n",
      "INFO:cell_load.data_modules.samplers:Creating meta-batches with cell_sentence_len=128...\n",
      "INFO:cell_load.data_modules.samplers:Of all batches, 120 were full and 0 were partial.\n",
      "INFO:cell_load.data_modules.samplers:Sampler created with 120 batches in 0.01 seconds.\n",
      "INFO:helical.models.state.state_train:Loading model from sample_vcc_data/first_run/final.ckpt\n",
      "INFO:helical.models.state.state_train:Model loaded successfully.\n",
      "INFO:helical.models.state.state_train:Generating predictions on test set using manual loop...\n",
      "Predicting:   0%|          | 0/120 [00:00<?, ?batch/s]INFO:cell_load.data_modules.samplers:Of all batches, 120 were full and 0 were partial.\n",
      "Predicting: 100%|██████████| 120/120 [00:06<00:00, 18.06batch/s]\n",
      "INFO:helical.models.state.state_train:Creating anndatas from predictions from manual loop...\n",
      "... storing 'target_gene' as categorical\n",
      "... storing 'cell_type' as categorical\n",
      "... storing 'batch_var' as categorical\n",
      "... storing 'ctrl_cell_barcode' as categorical\n",
      "... storing 'target_gene' as categorical\n",
      "... storing 'cell_type' as categorical\n",
      "... storing 'batch_var' as categorical\n",
      "... storing 'ctrl_cell_barcode' as categorical\n",
      "INFO:helical.models.state.state_train:Saved adata_pred to sample_vcc_data/adata_pred.h5ad\n",
      "INFO:helical.models.state.state_train:Saved adata_real to sample_vcc_data/adata_real.h5ad\n",
      "INFO:helical.models.state.state_train:Computing metrics using cell-eval...\n",
      "WARNING:helical.models.state.model_dir.vcc_eval._evaluator:Output directory sample_vcc_data already exists, potential overwrite occurring\n",
      "INFO:helical.models.state.model_dir.vcc_eval._evaluator:Input is found to be log-normalized already - skipping transformation.\n",
      "INFO:helical.models.state.model_dir.vcc_eval._evaluator:Input is found to be log-normalized already - skipping transformation.\n",
      "INFO:helical.models.state.model_dir.vcc_eval._evaluator:Computing DE for real data\n",
      "INFO:pdex._single_cell:Precomputing masks for each target gene\n",
      "Identifying target masks: 100%|██████████| 69/69 [00:00<00:00, 50622.18it/s]\n",
      "INFO:pdex._single_cell:Precomputing variable indices for each feature\n",
      "Identifying variable indices: 100%|██████████| 18080/18080 [00:00<00:00, 8342466.04it/s]\n",
      "INFO:pdex._single_cell:Creating shared memory memory matrix for parallel computing\n",
      "INFO:pdex._single_cell:Creating generator of all combinations: N=1247520\n",
      "INFO:pdex._single_cell:Creating generator of all batches: N=610\n",
      "INFO:pdex._single_cell:Initializing parallel processing pool\n",
      "INFO:pdex._single_cell:Processing batches\n",
      "Processing batches: 100%|██████████| 610/610 [00:22<00:00, 26.74it/s]\n",
      "INFO:pdex._single_cell:Flattening results\n",
      "INFO:pdex._single_cell:Closing shared memory pool\n",
      "INFO:helical.models.state.model_dir.vcc_eval._evaluator:Writing real DE results to: hepg2_real_de.csv\n",
      "INFO:helical.models.state.model_dir.vcc_eval._evaluator:Computing DE for pred data\n",
      "INFO:pdex._single_cell:Precomputing masks for each target gene\n",
      "Identifying target masks: 100%|██████████| 69/69 [00:00<00:00, 43143.56it/s]\n",
      "INFO:pdex._single_cell:Precomputing variable indices for each feature\n",
      "Identifying variable indices: 100%|██████████| 18080/18080 [00:00<00:00, 7789729.46it/s]\n",
      "INFO:pdex._single_cell:Creating shared memory memory matrix for parallel computing\n",
      "INFO:pdex._single_cell:Creating generator of all combinations: N=1247520\n",
      "INFO:pdex._single_cell:Creating generator of all batches: N=610\n",
      "INFO:pdex._single_cell:Initializing parallel processing pool\n",
      "INFO:pdex._single_cell:Processing batches\n",
      "Processing batches: 100%|██████████| 610/610 [00:24<00:00, 24.52it/s]\n",
      "INFO:pdex._single_cell:Flattening results\n",
      "INFO:pdex._single_cell:Closing shared memory pool\n",
      "INFO:helical.models.state.model_dir.vcc_eval._evaluator:Writing pred DE results to: hepg2_pred_de.csv\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'overlap_at_N'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'overlap_at_50'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'overlap_at_100'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'overlap_at_200'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'overlap_at_500'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'precision_at_N'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'precision_at_50'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'precision_at_100'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'precision_at_200'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'precision_at_500'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'de_spearman_sig'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'de_direction_match'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'de_spearman_lfc_sig'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'de_sig_genes_recall'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'de_nsig_counts'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'pr_auc'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'roc_auc'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'pearson_delta'\n",
      "INFO:helical.models.state.model_dir.vcc_eval._types._anndata:Building pseudobulk embeddings for real anndata on: .X\n",
      "INFO:helical.models.state.model_dir.vcc_eval._types._anndata:Building pseudobulk embeddings for predicted anndata on: .X\n",
      "Iterating over perturbations...: 100%|██████████| 68/68 [00:00<00:00, 1130.17it/s]\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'mse'\n",
      "Iterating over perturbations...: 100%|██████████| 68/68 [00:00<00:00, 4672.55it/s]\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'mae'\n",
      "Iterating over perturbations...: 100%|██████████| 68/68 [00:00<00:00, 5028.61it/s]\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'mse_delta'\n",
      "Iterating over perturbations...: 100%|██████████| 68/68 [00:00<00:00, 4990.34it/s]\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'mae_delta'\n",
      "Iterating over perturbations...: 100%|██████████| 68/68 [00:00<00:00, 5117.39it/s]\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'discrimination_score_l1'\n",
      "Iterating over perturbations...: 100%|██████████| 68/68 [00:00<00:00, 22249.21it/s]\n",
      "Iterating over perturbations...: 100%|██████████| 68/68 [00:00<00:00, 21964.78it/s]\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'discrimination_score_l2'\n",
      "Iterating over perturbations...: 100%|██████████| 68/68 [00:00<00:00, 23925.23it/s]\n",
      "Iterating over perturbations...: 100%|██████████| 68/68 [00:00<00:00, 21216.45it/s]\n",
      "INFO:helical.models.state.model_dir.vcc_eval._pipeline._runner:Computing metric 'discrimination_score_cosine'\n",
      "Iterating over perturbations...: 100%|██████████| 68/68 [00:00<00:00, 22630.54it/s]\n",
      "Iterating over perturbations...: 100%|██████████| 68/68 [00:00<00:00, 24341.78it/s]\n",
      "INFO:helical.models.state.model_dir.vcc_eval._evaluator:Writing perturbation level metrics to sample_vcc_data/hepg2_results.csv\n",
      "INFO:helical.models.state.model_dir.vcc_eval._evaluator:Writing aggregate metrics to sample_vcc_data/hepg2_agg_results.csv\n"
     ]
    }
   ],
   "source": [
    "# we can then train the model and perform inference on a held out test set\n",
    "state_train = stateTransitionTrainModel(configurer=train_config)\n",
    "state_train.train() \n",
    "state_train.predict() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb512a0",
   "metadata": {},
   "source": [
    "The trained model will be saved to the `sample_vcc_data/first_run` directory, alongside the necessary files and checkpoints to intialise a new model. We can initialise `stateTransitionModel` as before and run inference. For consistency with the `stateConfig` copy the `config.yaml` to the directory set as `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be67236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.state.state_transition:Using checkpoint: sample_vcc_data/first_run/final.ckpt\n",
      "INFO:helical.models.state.state_transition:Model device: cpu\n",
      "INFO:helical.models.state.state_transition:Model cell_set_len (max sequence length): 128\n",
      "INFO:helical.models.state.state_transition:Model uses batch encoder: False\n",
      "INFO:helical.models.state.state_transition:Model output space: all\n",
      "INFO:helical.models.state.state_transition:Grouping by cell type column: cell_type\n",
      "INFO:helical.models.state.state_transition:Using adata.X as input features\n",
      "INFO:helical.models.state.state_transition:Cells: total=98927, control=38176, non-control=60751\n",
      "INFO:helical.models.state.state_transition:Running virtual experiment (homogeneous per-perturbation forward passes; controls included)...\n",
      "Group H1: 100%|██████████| 51/51 [00:31<00:00,  1.64it/s, Pert: non-targeting           ]\n",
      "INFO:helical.models.state.state_transition:--Complete--\n",
      "Input cells: 98927, Control simulated: 38176, Treated simulated: 60751\n",
      "INFO:helical.models.state.state_transition:Wrote predictions to adata.obsm.perturbed_embeds in output file\n"
     ]
    }
   ],
   "source": [
    "from helical.models.state import stateTransitionModel\n",
    "from helical.models.state import stateConfig\n",
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad(\"sample_vcc_data/test.h5ad\")\n",
    "\n",
    "state_config = stateConfig(\n",
    "    output_path = \"sample_vcc_data/prediction.h5ad\",\n",
    "    perturb_dir = \"sample_vcc_data/first_run\",\n",
    "    pert_col = \"target_gene\",\n",
    ")\n",
    "\n",
    "state_transition = stateTransitionModel(configurer=state_config)\n",
    "processed_data = state_transition.process_data(adata)\n",
    "embeds = state_transition.get_embeddings(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45529597",
   "metadata": {},
   "source": [
    "Now you can use the `cell-eval` package to create a submission to the Virtual Cell Challenge (generates a `.vcc` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a629a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install cell-eval\n",
    "! cell-eval prep -i sample_vcc_data/prediction.h5ad -g sample_vcc_data/gene_names.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
