{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f5b3bc",
   "metadata": {},
   "source": [
    "This notebook goes over how to use `STATE` using `helical`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cbea7",
   "metadata": {},
   "source": [
    "# Download Example Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a40e1b8",
   "metadata": {},
   "source": [
    "We start by using the helical downloader to obtain an example huggingface dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed699842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.utils.downloader import Downloader\n",
    "from pathlib import Path\n",
    "\n",
    "downloader = Downloader()\n",
    "downloader.download_via_link(\n",
    "    Path(\"yolksac_human.h5ad\"),\n",
    "    \"https://huggingface.co/datasets/helical-ai/yolksac_human/resolve/main/data/17_04_24_YolkSacRaw_F158_WE_annots.h5ad?download=true\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e8491",
   "metadata": {},
   "source": [
    "# STATE Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ed3a3",
   "metadata": {},
   "source": [
    "Using the STATE model we can obtain single cell transcriptome embeddings. We first slice the dataset for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data \n",
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad(\"yolksac_human.h5ad\")\n",
    "# for demonstration we subset to 10 cells and 2000 genes\n",
    "adata = adata[:10, :2000].copy()\n",
    "\n",
    "print(adata.shape)\n",
    "n_cells = adata.n_obs\n",
    "print(n_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1c1b5",
   "metadata": {},
   "source": [
    "Initialise the model - this will download the relevant files needed in `.cache/helical/state/`. It will download the necessary files when run the first time so will take slightly longer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e73cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.models.state import stateConfig\n",
    "from helical.models.state import stateEmbed\n",
    "\n",
    "state_config = stateConfig(batch_size=16)\n",
    "state_embed = stateEmbed(configurer=state_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b4c72",
   "metadata": {},
   "source": [
    "We process the data by calling `state_embed.process_data` and pass this into `state_embed.get_embeddings` to get the final embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b1f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = state_embed.process_data(adata=adata)\n",
    "embeddings = state_embed.get_embeddings(processed_data)\n",
    "\n",
    "# note that the STATE model returns a numpy array of shape (n_cells, 1024)\n",
    "print(embeddings.shape)\n",
    "print(type(embeddings))\n",
    "\n",
    "# store the embeddings in adata.obsm['state_emb']\n",
    "adata.obsm['state_emb'] = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861511f",
   "metadata": {},
   "source": [
    "# STATE Perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e43c1bb",
   "metadata": {},
   "source": [
    "To use the perturbation model you can either pass in embeddings by specifiyng the `embed_key` arguement in `stateConfig` or use the deafult `None` value in which case the expression values are used (`adata.X`).\n",
    "\n",
    "For use of previous embeddings, the `embed_key` must exist in `adata.obsm[<embed_key>]` otherwise an error will be thrown. When set to `None` the model uses `adata.X`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6cd96",
   "metadata": {},
   "source": [
    "Let's create some dummy data for the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abafa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# some default control and non-control perturbations\n",
    "perturbations = [\n",
    "    \"[('DMSO_TF', 0.0, 'uM')]\",  # Control\n",
    "    \"[('Aspirin', 0.5, 'uM')]\",\n",
    "    \"[('Dexamethasone', 1.0, 'uM')]\",\n",
    "]\n",
    "\n",
    "n_cells = adata.n_obs\n",
    "# we assign perturbations to cells randomly\n",
    "adata.obs['target_gene'] = np.random.choice(perturbations, size=n_cells)\n",
    "adata.obs['cell_type'] = adata.obs['LVL1']  # Use your cell type column\n",
    "# we can also add a batch variable to take into account batch effects\n",
    "batch_labels = np.random.choice(['batch_1', 'batch_2', 'batch_3', 'batch_4'], size=n_cells)\n",
    "adata.obs['batch_var'] = batch_labels\n",
    "\n",
    "config = stateConfig(\n",
    "    embed_key=None,\n",
    "    pert_col=\"target_gene\",\n",
    "    celltype_col=\"cell_type\",\n",
    "    control_pert=\"[('DMSO_TF', 0.0, 'uM')]\",\n",
    "    output_path=\"yolksac_perturbed.h5ad\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c11cfb",
   "metadata": {},
   "source": [
    "Now we can run the perturbation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab93647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.models.state import stateTransitionModel\n",
    "\n",
    "state_transition = stateTransitionModel(configurer=config)\n",
    "\n",
    "# again we process the data and get the perturbed embeddings\n",
    "processed_data = state_transition.process_data(adata)\n",
    "perturbed_embeds = state_transition.get_embeddings(processed_data)\n",
    "\n",
    "print(perturbed_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582345dd",
   "metadata": {},
   "source": [
    "# Finetuning STATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e835b83",
   "metadata": {},
   "source": [
    "We can finetune the STATE perturbation embeddings using an additional head for downstream classification and regression. Below is a dummy example using data above to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helical.models.state import stateFineTuningModel\n",
    "\n",
    "# Dummy cell types and labels for demonstration\n",
    "cell_types = list(adata.obs['LVL1'])\n",
    "label_set = set(cell_types)\n",
    "print(f\"Found {len(label_set)} unique cell types:\")\n",
    "\n",
    "config = stateConfig(\n",
    "    embed_key=None,\n",
    "    pert_col=\"target_gene\",\n",
    "    celltype_col=\"cell_type\",\n",
    "    control_pert=\"[('DMSO_TF', 0.0, 'uM')]\",\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "# Create the fine-tuning model - we use a classification head for demonstration\n",
    "model = stateFineTuningModel(\n",
    "    configurer=config, \n",
    "    fine_tuning_head=\"classification\", \n",
    "    output_size=len(label_set),\n",
    ")\n",
    "\n",
    "# Process the data for training - returns a dataset object\n",
    "data = model.process_data(adata)\n",
    "\n",
    "# Create a dictionary mapping the classes to unique integers for training\n",
    "class_id_dict = dict(zip(label_set, [i for i in range(len(label_set))]))\n",
    "\n",
    "# Convert cell type labels to integers\n",
    "cell_type_labels = [class_id_dict[ct] for ct in cell_types]\n",
    "\n",
    "print(f\"Class mapping: {class_id_dict}\")\n",
    "\n",
    "# Fine-tune\n",
    "model.train(train_input_data=data, train_labels=cell_type_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53936df",
   "metadata": {},
   "source": [
    "# Training STATE for the Virtual Cell Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be814ae",
   "metadata": {},
   "source": [
    "We use data from the Virtual Cell Challenge for model training and downstream inference. For this we require the VCC dataset as in the colab notebook by the authors. See the relevant code snippet for the entire dataset in the below colab notebook:\n",
    "\n",
    "[STATE Colab Notebook](https://colab.research.google.com/drive/1QKOtYP7bMpdgDJEipDxaJqOchv7oQ-_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e7ad8",
   "metadata": {},
   "source": [
    "For demonstration we have created a subset of the data. We also need to change the filepath in `starter.toml` to point to the correct dataset location (see top of file), but this is done below in the code. Start by downloading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83609bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:helical.utils.downloader:File size mismatch: local 210, remote 465\n",
      "WARNING:helical.utils.downloader:File '/home/rasched/.cache/helical/models/state/sample_vcc_data/starter.toml' is corrupted or invalid. Deleting and re-downloading.\n",
      "INFO:helical.utils.downloader:Downloading 'state/sample_vcc_data/starter.toml'\n",
      "INFO:helical.utils.downloader:Starting to download: 'https://helicalpackage.s3.eu-west-2.amazonaws.com/state/sample_vcc_data/starter.toml'\n",
      "starter.toml: 100%|██████████| 465/465 [00:00<00:00, 4.71MB/s]\n",
      "INFO:helical.utils.downloader:File saved to: '/home/rasched/.cache/helical/models/state/sample_vcc_data/starter.toml'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[datasets]\\nreplogle_h1 = \"/home/rasched/.cache/helical/models/state/sample_vcc_data/{rpe1_mini,hepg2_mini}.h5\"\\n\\n[training]\\nreplogle_h1 = \"train\"\\n\\n[zeroshot]\\n\"replogle_h1.hepg2\" = \"test\"\\n\\n[fewshot]\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helical.utils.downloader import Downloader\n",
    "from helical.constants.paths import CACHE_DIR_HELICAL\n",
    "import toml\n",
    "from pathlib import Path\n",
    "\n",
    "downloader = Downloader()\n",
    "downloader.download_via_name(\"state/sample_vcc_data/config.yaml\")\n",
    "downloader.download_via_name(\"state/sample_vcc_data/starter.toml\")\n",
    "downloader.download_via_name(\"state/sample_vcc_data/gene_names.csv\")\n",
    "downloader.download_via_name(\"state/sample_vcc_data/ESM2_pert_features.pt\")\n",
    "downloader.download_via_name(\"state/sample_vcc_data/hepg2_mini.h5\")\n",
    "downloader.download_via_name(\"state/sample_vcc_data/rpe1_mini.h5\")\n",
    "downloader.download_via_name(\"state/sample_vcc_data/test.h5ad\")\n",
    "\n",
    "toml.dump({**toml.load(open(Path(CACHE_DIR_HELICAL, \"state/sample_vcc_data/starter.toml\"))),**{\"datasets\": {\"replogle_h1\": str(Path(CACHE_DIR_HELICAL, \"state/sample_vcc_data\").absolute() / \"{rpe1_mini,hepg2_mini}.h5\")}},},open(Path(CACHE_DIR_HELICAL, \"state/sample_vcc_data/starter.toml\"), \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974174ac",
   "metadata": {},
   "source": [
    "We use the `stateTransitionTrainModel` class and initialise training configurations using the `config.yaml` file in the sample directory. You can edit these based on your training preferences. Currently this is set to one epoch for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23307fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:Seed set to 42\n",
      "WARNING:cell_load.config:Dataset path does not exist: /home/rasched/.cache/helical/models/state/sample_vcc_data/{rpe1_mini,hepg2_mini}.h5\n",
      "INFO:cell_load.config:Configuration validation passed\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:Initializing DataModule: batch_size=16, workers=4, random_seed=42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rasched/.cache/helical/models/state/sample_vcc_data/{rpe1_mini,hepg2_mini}.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cell_load.data_modules.perturbation_dataloader:Set 2 missing perturbations to zero vectors.\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:Loaded custom perturbation featurizations for 19792 perturbations.\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:Processing dataset replogle_h1:\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:  - Training dataset: True\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:  - Zeroshot cell types: ['hepg2']\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:  - Fewshot cell types: []\n",
      "Processing replogle_h1:   0%|          | 0/2 [00:00<?, ?it/s]WARNING:cell_load.dataset._perturbation:No cell barcode information found in /home/rasched/.cache/helical/models/state/sample_vcc_data/rpe1_mini.h5. Generating generic barcodes.\n",
      "Processing replogle_h1:   0%|          | 0/2 [00:00<?, ?it/s]WARNING:cell_load.dataset._perturbation:No cell barcode information found in /home/rasched/.cache/helical/models/state/sample_vcc_data/hepg2_mini.h5. Generating generic barcodes.\n",
      "Processing replogle_h1: 100%|██████████| 2/2 [00:00<00:00, 403.28it/s]\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:\n",
      "\n",
      "INFO:cell_load.data_modules.perturbation_dataloader:Done! Train / Val / Test splits: 1 / 0 / 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed rpe1_mini: 100 train, 0 val, 0 test\n",
      "Processed hepg2_mini: 0 train, 0 val, 100 test\n",
      "Model created. Estimated params size: 0.61 GB and 650505936 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helical.models.state.state_train:Loggers and callbacks set up.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:helical.models.state.state_train:Starting trainer fit.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer built successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: \n",
      "  | Name                 | Type                    | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | loss_fn              | SamplesLoss             | 0      | train\n",
      "1 | pert_encoder         | Sequential              | 4.8 M  | train\n",
      "2 | basal_encoder        | Linear                  | 12.2 M | train\n",
      "3 | transformer_backbone | LlamaBidirectionalModel | 50.4 M | train\n",
      "4 | project_out          | Sequential              | 13.5 M | train\n",
      "5 | final_down_then_up   | Sequential              | 81.7 M | train\n",
      "6 | relu                 | ReLU                    | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "141 M     Trainable params\n",
      "21.5 M    Non-trainable params\n",
      "162 M     Total params\n",
      "650.506   Total estimated model params size (MB)\n",
      "86        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name                 | Type                    | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | loss_fn              | SamplesLoss             | 0      | train\n",
      "1 | pert_encoder         | Sequential              | 4.8 M  | train\n",
      "2 | basal_encoder        | Linear                  | 12.2 M | train\n",
      "3 | transformer_backbone | LlamaBidirectionalModel | 50.4 M | train\n",
      "4 | project_out          | Sequential              | 13.5 M | train\n",
      "5 | final_down_then_up   | Sequential              | 81.7 M | train\n",
      "6 | relu                 | ReLU                    | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "141 M     Trainable params\n",
      "21.5 M    Non-trainable params\n",
      "162 M     Total params\n",
      "650.506   Total estimated model params size (MB)\n",
      "86        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cell_load.data_modules.samplers:Creating perturbation batch sampler with metadata caching (using codes)...\n",
      "INFO:cell_load.data_modules.samplers:Total # cells 100. Cell set size mean / std before resampling: 4.76 / 11.85.\n",
      "INFO:cell_load.data_modules.samplers:Creating meta-batches with cell_sentence_len=128...\n",
      "INFO:cell_load.data_modules.samplers:Of all batches, 0 were full and 21 were partial.\n",
      "INFO:cell_load.data_modules.samplers:Sampler created with 2 batches in 0.00 seconds.\n",
      "INFO:cell_load.data_modules.samplers:Of all batches, 0 were full and 21 were partial.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cell_load.data_modules.samplers:Creating perturbation batch sampler with metadata caching (using codes)...\n",
      "INFO:cell_load.data_modules.samplers:Total # cells 100. Cell set size mean / std before resampling: 4.55 / 12.04.\n",
      "INFO:cell_load.data_modules.samplers:Creating meta-batches with cell_sentence_len=128...\n",
      "INFO:cell_load.data_modules.samplers:Of all batches, 0 were full and 22 were partial.\n",
      "INFO:cell_load.data_modules.samplers:Sampler created with 2 batches in 0.00 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cell_load.data_modules.samplers:Of all batches, 0 were full and 22 were partial.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s, v_num=0]\n",
      "Training completed, saving final checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cell_load.data_modules.samplers:Creating perturbation batch sampler with metadata caching (using codes)...\n",
      "INFO:cell_load.data_modules.samplers:Total # cells 100. Cell set size mean / std before resampling: 4.76 / 11.85.\n",
      "INFO:cell_load.data_modules.samplers:Creating meta-batches with cell_sentence_len=128...\n",
      "INFO:cell_load.data_modules.samplers:Of all batches, 21 were full and 0 were partial.\n",
      "INFO:cell_load.data_modules.samplers:Sampler created with 21 batches in 0.00 seconds.\n",
      "INFO:helical.models.state.state_train:Loading model from sample_run/first_run/final.ckpt\n",
      "INFO:helical.models.state.state_train:Model loaded successfully.\n",
      "INFO:helical.models.state.state_train:Generating predictions on test set using manual loop...\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?batch/s]INFO:cell_load.data_modules.samplers:Of all batches, 21 were full and 0 were partial.\n",
      "Predicting: 100%|██████████| 21/21 [00:00<00:00, 23.41batch/s]\n",
      "INFO:helical.models.state.state_train:Creating anndatas from predictions from manual loop...\n",
      "... storing 'target_gene' as categorical\n",
      "... storing 'cell_type' as categorical\n",
      "... storing 'batch_var' as categorical\n",
      "... storing 'ctrl_cell_barcode' as categorical\n",
      "... storing 'target_gene' as categorical\n",
      "... storing 'cell_type' as categorical\n",
      "... storing 'batch_var' as categorical\n",
      "... storing 'ctrl_cell_barcode' as categorical\n",
      "INFO:helical.models.state.state_train:Saved adata_pred to sample_run/adata_pred.h5ad\n",
      "INFO:helical.models.state.state_train:Saved adata_real to sample_run/adata_real.h5ad\n",
      "INFO:helical.models.state.state_train:Computing metrics using cell-eval...\n",
      "WARNING:helical.models.state.model_dir.train_utils.evaluator:Output directory sample_run already exists, potential overwrite occurring\n",
      "INFO:helical.models.state.model_dir.train_utils.evaluator:Input is found to be log-normalized already - skipping transformation.\n",
      "INFO:helical.models.state.model_dir.train_utils.evaluator:Input is found to be log-normalized already - skipping transformation.\n",
      "INFO:helical.models.state.model_dir.train_utils.evaluator:Computing DE for real data\n",
      "INFO:pdex._single_cell:Precomputing masks for each target gene\n",
      "Identifying target masks: 100%|██████████| 21/21 [00:00<00:00, 42204.30it/s]\n",
      "INFO:pdex._single_cell:Precomputing variable indices for each feature\n",
      "Identifying variable indices: 100%|██████████| 18080/18080 [00:00<00:00, 6629919.24it/s]\n",
      "INFO:pdex._single_cell:Creating shared memory memory matrix for parallel computing\n",
      "INFO:pdex._single_cell:Creating generator of all combinations: N=379680\n",
      "INFO:pdex._single_cell:Creating generator of all batches: N=186\n",
      "INFO:pdex._single_cell:Initializing parallel processing pool\n",
      "INFO:pdex._single_cell:Processing batches\n",
      "Processing batches: 100%|██████████| 186/186 [00:04<00:00, 46.20it/s]\n",
      "INFO:pdex._single_cell:Flattening results\n",
      "INFO:pdex._single_cell:Closing shared memory pool\n",
      "INFO:helical.models.state.model_dir.train_utils.evaluator:Writing real DE results to: hepg2_real_de.csv\n",
      "INFO:helical.models.state.model_dir.train_utils.evaluator:Computing DE for pred data\n",
      "INFO:pdex._single_cell:Precomputing masks for each target gene\n",
      "Identifying target masks: 100%|██████████| 21/21 [00:00<00:00, 36961.97it/s]\n",
      "INFO:pdex._single_cell:Precomputing variable indices for each feature\n",
      "Identifying variable indices: 100%|██████████| 18080/18080 [00:00<00:00, 7449215.75it/s]\n",
      "INFO:pdex._single_cell:Creating shared memory memory matrix for parallel computing\n",
      "INFO:pdex._single_cell:Creating generator of all combinations: N=379680\n",
      "INFO:pdex._single_cell:Creating generator of all batches: N=186\n",
      "INFO:pdex._single_cell:Initializing parallel processing pool\n",
      "INFO:pdex._single_cell:Processing batches\n",
      "Processing batches: 100%|██████████| 186/186 [00:04<00:00, 44.57it/s]\n",
      "INFO:pdex._single_cell:Flattening results\n",
      "INFO:pdex._single_cell:Closing shared memory pool\n",
      "INFO:helical.models.state.model_dir.train_utils.evaluator:Writing pred DE results to: hepg2_pred_de.csv\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'overlap_at_N'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'overlap_at_50'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'overlap_at_100'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'overlap_at_200'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'overlap_at_500'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'precision_at_N'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'precision_at_50'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'precision_at_100'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'precision_at_200'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'precision_at_500'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'de_spearman_sig'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'de_direction_match'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'de_spearman_lfc_sig'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'de_sig_genes_recall'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'de_nsig_counts'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'pr_auc'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'roc_auc'\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'pearson_delta'\n",
      "INFO:helical.models.state.model_dir.train_utils._types._anndata:Building pseudobulk embeddings for real anndata on: .X\n",
      "INFO:helical.models.state.model_dir.train_utils._types._anndata:Building pseudobulk embeddings for predicted anndata on: .X\n",
      "Iterating over perturbations...: 100%|██████████| 20/20 [00:00<00:00, 3859.85it/s]\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'mse'\n",
      "Iterating over perturbations...: 100%|██████████| 20/20 [00:00<00:00, 5471.66it/s]\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'mae'\n",
      "Iterating over perturbations...: 100%|██████████| 20/20 [00:00<00:00, 5968.84it/s]\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'mse_delta'\n",
      "Iterating over perturbations...: 100%|██████████| 20/20 [00:00<00:00, 6523.53it/s]\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'mae_delta'\n",
      "Iterating over perturbations...: 100%|██████████| 20/20 [00:00<00:00, 6631.31it/s]\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'discrimination_score_l1'\n",
      "Iterating over perturbations...: 100%|██████████| 20/20 [00:00<00:00, 28273.03it/s]\n",
      "Iterating over perturbations...: 100%|██████████| 20/20 [00:00<00:00, 36711.63it/s]\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'discrimination_score_l2'\n",
      "Iterating over perturbations...: 100%|██████████| 20/20 [00:00<00:00, 31347.56it/s]\n",
      "Iterating over perturbations...: 100%|██████████| 20/20 [00:00<00:00, 36314.32it/s]\n",
      "INFO:helical.models.state.model_dir.train_utils._pipeline._runner:Computing metric 'discrimination_score_cosine'\n",
      "Iterating over perturbations...: 100%|██████████| 20/20 [00:00<00:00, 25700.39it/s]\n",
      "Iterating over perturbations...: 100%|██████████| 20/20 [00:00<00:00, 27025.15it/s]\n",
      "INFO:helical.models.state.model_dir.train_utils.evaluator:Writing perturbation level metrics to sample_run/hepg2_results.csv\n",
      "INFO:helical.models.state.model_dir.train_utils.evaluator:Writing aggregate metrics to sample_run/hepg2_agg_results.csv\n"
     ]
    }
   ],
   "source": [
    "# we can then train the model and perform inference on a held out test set\n",
    "from helical.models.state import stateTransitionTrainModel\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "train_configs = OmegaConf.load(Path(CACHE_DIR_HELICAL, \"state/sample_vcc_data/config.yaml\"))\n",
    "# set the correct paths for the data\n",
    "train_configs.data.kwargs.toml_config_path = Path(CACHE_DIR_HELICAL , \"state/sample_vcc_data/starter.toml\")\n",
    "train_configs.data.kwargs.perturbation_features_file = Path(CACHE_DIR_HELICAL , \"state/sample_vcc_data/ESM2_pert_features.pt\")\n",
    "\n",
    "state_train = stateTransitionTrainModel(configurer=train_configs)\n",
    "state_train.train() \n",
    "state_train.predict() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb512a0",
   "metadata": {},
   "source": [
    "The trained model will be saved to the `sample_vcc_data/first_run` directory, alongside the necessary files and checkpoints to intialise a new model. We can initialise `stateTransitionModel` as before and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be67236",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'sample_vcc_data/test.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhelical\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stateConfig\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscanpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msc\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m adata = \u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_h5ad\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msample_vcc_data/test.h5ad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m state_config = stateConfig(\n\u001b[32m      8\u001b[39m     output_path = \u001b[33m\"\u001b[39m\u001b[33msample_run/prediction.h5ad\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     perturb_dir = \u001b[33m\"\u001b[39m\u001b[33msample_run/first_run\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     pert_col = \u001b[33m\"\u001b[39m\u001b[33mtarget_gene\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m state_transition = stateTransitionModel(configurer=state_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/helical/lib/python3.11/site-packages/anndata/_io/h5ad.py:263\u001b[39m, in \u001b[36mread_h5ad\u001b[39m\u001b[34m(filename, backed, as_sparse, as_sparse_fmt, chunk_size)\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m    259\u001b[39m rdasp = partial(\n\u001b[32m    260\u001b[39m     read_dense_as_sparse, sparse_format=as_sparse_fmt, axis_chunk=chunk_size\n\u001b[32m    261\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcallback\u001b[39m(func, elem_name: \u001b[38;5;28mstr\u001b[39m, elem, iospec):\n\u001b[32m    266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m iospec.encoding_type == \u001b[33m\"\u001b[39m\u001b[33manndata\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_name.endswith(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/helical/lib/python3.11/site-packages/h5py/_hl/files.py:564\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[39m\n\u001b[32m    555\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[32m    556\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[32m    557\u001b[39m                      alignment_threshold=alignment_threshold,\n\u001b[32m    558\u001b[39m                      alignment_interval=alignment_interval,\n\u001b[32m    559\u001b[39m                      meta_block_size=meta_block_size,\n\u001b[32m    560\u001b[39m                      **kwds)\n\u001b[32m    561\u001b[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[32m    562\u001b[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n\u001b[32m    563\u001b[39m                      fs_page_size=fs_page_size)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/helical/lib/python3.11/site-packages/h5py/_hl/files.py:238\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[32m    237\u001b[39m         flags |= h5f.ACC_SWMR_READ\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    240\u001b[39m     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:102\u001b[39m, in \u001b[36mh5py.h5f.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'sample_vcc_data/test.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from helical.models.state import stateTransitionModel\n",
    "from helical.models.state import stateConfig\n",
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad(\"sample_vcc_data/test.h5ad\")\n",
    "\n",
    "state_config = stateConfig(\n",
    "    output_path = \"sample_run/prediction.h5ad\",\n",
    "    perturb_dir = \"sample_run/first_run\",\n",
    "    pert_col = \"target_gene\",\n",
    ")\n",
    "\n",
    "state_transition = stateTransitionModel(configurer=state_config)\n",
    "processed_data = state_transition.process_data(adata)\n",
    "embeds = state_transition.get_embeddings(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45529597",
   "metadata": {},
   "source": [
    "Now you can use the `cell-eval` package to create a submission to the Virtual Cell Challenge (generates a `.vcc` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a629a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install cell-eval\n",
    "! cell-eval prep -i sample_run/prediction.h5ad -g sample_vcc_data/gene_names.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3902e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scanpy as sc\n",
    "# import numpy as np\n",
    "\n",
    "# def create_balanced_mini_dataset(input_path, output_path, n_cells=100):\n",
    "#     \"\"\"\n",
    "#     Create a mini dataset that preserves both control and perturbation cells\n",
    "#     \"\"\"\n",
    "#     adata = sc.read_h5ad(input_path)\n",
    "    \n",
    "#     # Find control and perturbation cells\n",
    "#     control_mask = adata.obs['target_gene'] == 'non-targeting'\n",
    "#     pert_mask = ~control_mask\n",
    "    \n",
    "#     control_indices = np.where(control_mask)[0]\n",
    "#     pert_indices = np.where(pert_mask)[0]\n",
    "    \n",
    "#     print(f\"Original: {len(control_indices)} control, {len(pert_indices)} perturbation cells\")\n",
    "    \n",
    "#     # Sample proportionally\n",
    "#     n_control = min(n_cells // 2, len(control_indices))\n",
    "#     n_pert = min(n_cells - n_control, len(pert_indices))\n",
    "    \n",
    "#     # If we need more cells, fill with the remaining type\n",
    "#     if n_control + n_pert < n_cells:\n",
    "#         if len(control_indices) > n_control:\n",
    "#             n_control = min(n_cells, len(control_indices))\n",
    "#             n_pert = 0\n",
    "#         elif len(pert_indices) > n_pert:\n",
    "#             n_pert = min(n_cells, len(pert_indices))\n",
    "#             n_control = 0\n",
    "    \n",
    "#     # Sample indices\n",
    "#     np.random.seed(42)\n",
    "#     sampled_control = np.random.choice(control_indices, size=n_control, replace=False) if n_control > 0 else np.array([])\n",
    "#     sampled_pert = np.random.choice(pert_indices, size=n_pert, replace=False) if n_pert > 0 else np.array([])\n",
    "    \n",
    "#     # Combine and create new dataset\n",
    "#     all_sampled = np.concatenate([sampled_control, sampled_pert])\n",
    "#     adata_mini = adata[all_sampled, :].copy()\n",
    "    \n",
    "#     print(f\"Mini dataset: {len(sampled_control)} control, {len(sampled_pert)} perturbation cells\")\n",
    "#     print(f\"Total: {adata_mini.shape}\")\n",
    "    \n",
    "#     adata_mini.write_h5ad(output_path)\n",
    "#     return adata_mini\n",
    "\n",
    "# # Create a balanced mini dataset\n",
    "# mini_val = create_balanced_mini_dataset(\"competition_support_set/competition_val_template.h5ad\", \"competition_support_set/mini_val_balanced.h5ad\", n_cells=100)\n",
    "\n",
    "# import scanpy as sc\n",
    "# import anndata as ad\n",
    "\n",
    "# def truncate_adata_file_complete(input_path, output_path, max_cells=100, max_genes=None):\n",
    "#     \"\"\"\n",
    "#     Truncate an AnnData file and handle ALL fields properly\n",
    "#     \"\"\"\n",
    "#     print(f\"Loading {input_path}...\")\n",
    "#     adata = sc.read_h5ad(input_path)\n",
    "    \n",
    "#     print(f\"Original shape: {adata.shape}\")\n",
    "#     print(f\"Original obsm keys: {list(adata.obsm.keys())}\")\n",
    "    \n",
    "#     # Truncate cells\n",
    "#     if max_cells and adata.n_obs > max_cells:\n",
    "#         print(f\"Truncating to {max_cells} cells...\")\n",
    "        \n",
    "#         # Truncate main data\n",
    "#         adata = adata[:max_cells, :].copy()\n",
    "        \n",
    "#         # Manually truncate obsm fields that might not be handled properly\n",
    "#         for key in adata.obsm.keys():\n",
    "#             matrix = adata.obsm[key]\n",
    "#             if hasattr(matrix, 'shape') and len(matrix.shape) > 0:\n",
    "#                 if matrix.shape[0] > max_cells:\n",
    "#                     print(f\"Truncating obsm['{key}'] from {matrix.shape} to ({max_cells}, {matrix.shape[1] if len(matrix.shape) > 1 else 'N/A'})\")\n",
    "#                     adata.obsm[key] = matrix[:max_cells]\n",
    "    \n",
    "#     # Truncate genes (optional)\n",
    "#     if max_genes and adata.n_vars > max_genes:\n",
    "#         print(f\"Truncating to {max_genes} genes...\")\n",
    "        \n",
    "#         # Truncate main data\n",
    "#         adata = adata[:, :max_genes].copy()\n",
    "        \n",
    "#         # Manually truncate varm fields\n",
    "#         for key in adata.varm.keys():\n",
    "#             matrix = adata.varm[key]\n",
    "#             if hasattr(matrix, 'shape') and len(matrix.shape) > 0:\n",
    "#                 if matrix.shape[0] > max_genes:\n",
    "#                     print(f\"Truncating varm['{key}'] from {matrix.shape} to ({max_genes}, {matrix.shape[1] if len(matrix.shape) > 1 else 'N/A'})\")\n",
    "#                     adata.varm[key] = matrix[:max_genes]\n",
    "    \n",
    "#     print(f\"New shape: {adata.shape}\")\n",
    "#     print(f\"New obsm keys: {list(adata.obsm.keys())}\")\n",
    "    \n",
    "#     # Save truncated file\n",
    "#     print(f\"Saving to {output_path}...\")\n",
    "#     adata.write_h5ad(output_path)\n",
    "    \n",
    "#     return adata\n",
    "\n",
    "# # Create mini version of rpe1.h5\n",
    "# rpe1_mini = truncate_adata_file_complete('sample_vcc_data/rpe1.h5', 'sample_vcc_data/rpe1_mini.h5', max_cells=100)\n",
    "# rpe1_mini = truncate_adata_file_complete('sample_vcc_data/hepg2.h5', 'sample_vcc_data/hepg2_mini.h5', max_cells=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
