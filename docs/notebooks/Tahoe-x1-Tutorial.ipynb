{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Tahoe-x1 Model Tutorial\n",
    "\n",
    "Run this notebook on a colab notebook with a free GPU:\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/helicalAI/helical/blob/main/examples/notebooks/Tahoe-x1-Tutorial.ipynb)\n",
    "\n",
    "This tutorial demonstrates how to use the Tahoe-x1 foundation model for single-cell RNA-seq data. Tahoe-x1 is a transformer-based model that can extract both cell and gene embeddings from raw count data.\n",
    "\n",
    "**What you'll learn in this notebook:**\n",
    "- How to load and configure the Tahoe-x1 model\n",
    "- Processing single-cell RNA-seq data for Tahoe\n",
    "- Extracting cell embeddings\n",
    "- Extracting gene embeddings\n",
    "- Visualizing embeddings with UMAP\n",
    "- Extracting attention weights for interpretability\n",
    "\n",
    "For more examples, check out our [GitHub](https://github.com/helicalAI/helical) and [documentation](https://helical.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasched/miniconda3/envs/helical/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check device availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## Load Example Dataset\n",
    "\n",
    "We'll use the human fetal yolk sac scRNA-seq dataset from Helical's Hugging Face repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 25344/25344 [00:15<00:00, 1648.67 examples/s]\n",
      "Generating test split: 100%|██████████| 6336/6336 [00:03<00:00, 2050.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2534 cells\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\n",
    "    \"helical-ai/yolksac_human\", \n",
    "    split=\"train[:10%]\", \n",
    "    trust_remote_code=True, \n",
    "    download_mode=\"reuse_cache_if_exists\"\n",
    ")\n",
    "\n",
    "# Store labels for visualization later\n",
    "labels = dataset[\"LVL1\"]\n",
    "\n",
    "print(f\"Loaded {len(dataset)} cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convert-anndata",
   "metadata": {},
   "source": [
    "## Convert to AnnData Format\n",
    "\n",
    "Tahoe works with AnnData objects, the standard format for single-cell data in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "create-anndata",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 2534 × 37318\n",
      "    obs: 'LVL1', 'LVL2', 'LVL3'\n",
      "    var: 'gene_name'\n",
      "\n",
      "Using subset: 500 cells, 37318 genes\n"
     ]
    }
   ],
   "source": [
    "from helical.utils import get_anndata_from_hf_dataset\n",
    "\n",
    "ann_data = get_anndata_from_hf_dataset(dataset)\n",
    "print(ann_data)\n",
    "\n",
    "# For this tutorial, let's use a subset for faster processing\n",
    "ann_data_subset = ann_data[:500]  # Use first 500 cells\n",
    "labels_subset = labels[:500]\n",
    "print(f\"\\nUsing subset: {ann_data_subset.n_obs} cells, {ann_data_subset.n_vars} genes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-setup",
   "metadata": {},
   "source": [
    "## Initialize Tahoe Model\n",
    "\n",
    "Tahoe comes in three sizes (70m, 1b, 3b). Currently, the 70m model is available. The model uses Flash Attention by default for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:58:44,709 - INFO:helical.models.tahoe.model:Loading Tahoe model (size: 70m) from Hugging Face...\n",
      "2025-12-02 15:58:45,399 - INFO:helical.models.tahoe.tahoe_x1.model.model:MosaicML recommends using config.init_device=\"meta\" with Composer + FSDP for faster initialization.\n",
      "2025-12-02 15:58:46,574 - INFO:helical.models.tahoe.model:Model loaded with 12 transformer layers.\n",
      "2025-12-02 15:58:46,575 - INFO:helical.models.tahoe.model:Tahoe model is in 'eval' mode, on device 'cuda' with embedding mode 'cell' and attention implementation 'flash'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tahoe model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from helical.models.tahoe import Tahoe, TahoeConfig\n",
    "\n",
    "# Configure the Tahoe model\n",
    "tahoe_config = TahoeConfig(\n",
    "    model_size=\"70m\",  # 12-layer transformer with 512 embedding dimensions\n",
    "    batch_size=8,      # Adjust based on your GPU memory\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Initialize the model (will download weights on first use)\n",
    "tahoe = Tahoe(configurer=tahoe_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process-data",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "\n",
    "Tahoe requires gene names to be mapped to Ensembl IDs. The `process_data` method handles this automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:00:31,198 - INFO:helical.models.tahoe.model:Processing data for Tahoe.\n",
      "2025-12-02 16:00:31,388 - INFO:helical.utils.mapping:Mapped 20969 / 37318 genes to Ensembl IDs.\n",
      "2025-12-02 16:00:31,401 - INFO:helical.models.tahoe.model:Matched 20865/37318 genes in vocabulary of size 62720.\n",
      "2025-12-02 16:00:31,527 - INFO:helical.models.tahoe.model:Successfully processed the data for Tahoe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed and ready for inference!\n"
     ]
    }
   ],
   "source": [
    "# Process data - this will map gene symbols to Ensembl IDs\n",
    "dataloader = tahoe.process_data(\n",
    "    ann_data_subset,\n",
    "    gene_names=\"gene_name\",  # Column containing gene symbols\n",
    "    use_raw_counts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-embeddings-section",
   "metadata": {},
   "source": [
    "## Extract Cell Embeddings\n",
    "\n",
    "Cell embeddings capture the transcriptional state of each cell in a dense vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "get-cell-embeddings",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:00:33,248 - INFO:helical.models.tahoe.model:Extracting embeddings from Tahoe model...\n",
      "Embedding cells: 100%|██████████| 63/63 [00:04<00:00, 18.03it/s]2025-12-02 16:00:37,507 - INFO:helical.models.tahoe.model:Finished extracting embeddings. Cell shape: (500, 512)\n",
      "Embedding cells: 100%|██████████| 63/63 [00:04<00:00, 14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell embeddings shape: (500, 512)\n",
      "Each cell is represented by a 512-dimensional vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get cell embeddings\n",
    "cell_embeddings = tahoe.get_embeddings(dataloader)\n",
    "\n",
    "print(f\"Cell embeddings shape: {cell_embeddings.shape}\")\n",
    "print(f\"Each cell is represented by a {cell_embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-umap",
   "metadata": {},
   "source": [
    "## Visualize Cell Embeddings with UMAP\n",
    "\n",
    "Let's visualize the cell embeddings in 2D using UMAP to see how cells cluster by cell type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "umap-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce dimensionality with UMAP\n",
    "reducer = umap.UMAP(min_dist=0.1, n_components=2, n_neighbors=15, random_state=42)\n",
    "umap_embedding = reducer.fit_transform(cell_embeddings)\n",
    "\n",
    "# Create plot dataframe\n",
    "plot_df = pd.DataFrame(umap_embedding, columns=['UMAP1', 'UMAP2'])\n",
    "plot_df['Cell Type'] = labels_subset\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=plot_df, \n",
    "    x='UMAP1', \n",
    "    y='UMAP2', \n",
    "    hue='Cell Type',\n",
    "    palette='tab10',\n",
    "    s=30,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('UMAP Visualization of Tahoe Cell Embeddings', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('UMAP 1', fontsize=12)\n",
    "plt.ylabel('UMAP 2', fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Cell Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gene-embeddings-section",
   "metadata": {},
   "source": [
    "## Extract Gene Embeddings\n",
    "\n",
    "Tahoe can also extract gene embeddings for each cell. Gene embeddings are returned as a **list of pandas Series** (one per cell), where each Series contains the embeddings for genes expressed in that specific cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-gene-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get both cell and gene embeddings\n",
    "cell_embeddings, gene_embeddings = tahoe.get_embeddings(\n",
    "    dataloader,\n",
    "    return_gene_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"Cell embeddings shape: {cell_embeddings.shape}\")\n",
    "print(f\"Gene embeddings: {len(gene_embeddings)} cells (list of pandas Series)\")\n",
    "print(f\"\\nFirst cell has {len(gene_embeddings[0])} genes expressed\")\n",
    "\n",
    "# Get first gene embedding from first cell\n",
    "first_gene_embedding = gene_embeddings[0].iloc[0]\n",
    "print(f\"Each gene has a {len(first_gene_embedding)}-dimensional embedding\")\n",
    "\n",
    "print(f\"\\nExample - First 5 genes in first cell:\")\n",
    "for gene_id, embedding in list(gene_embeddings[0].items())[:5]:\n",
    "    print(f\"  {gene_id}: shape {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gene-viz-section",
   "metadata": {},
   "source": [
    "## Visualize Gene Embeddings\n",
    "\n",
    "Gene embeddings are returned as a list of pandas Series (one per cell), where each Series contains gene embeddings for genes expressed in that cell. Let's aggregate and visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-genes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate gene embeddings across all cells (average per gene)\n",
    "from collections import defaultdict\n",
    "\n",
    "gene_embedding_accumulator = defaultdict(lambda: {'sum': None, 'count': 0})\n",
    "\n",
    "# Accumulate embeddings for each gene across all cells\n",
    "for cell_series in gene_embeddings:\n",
    "    for gene_id, embedding in cell_series.items():\n",
    "        if gene_embedding_accumulator[gene_id]['sum'] is None:\n",
    "            gene_embedding_accumulator[gene_id]['sum'] = embedding.copy()\n",
    "        else:\n",
    "            gene_embedding_accumulator[gene_id]['sum'] += embedding\n",
    "        gene_embedding_accumulator[gene_id]['count'] += 1\n",
    "\n",
    "# Average the embeddings\n",
    "aggregated_gene_embeddings = {}\n",
    "for gene_id, data in gene_embedding_accumulator.items():\n",
    "    aggregated_gene_embeddings[gene_id] = data['sum'] / data['count']\n",
    "\n",
    "print(f\"Aggregated gene embeddings for {len(aggregated_gene_embeddings)} unique genes\")\n",
    "\n",
    "# Convert to numpy array for visualization\n",
    "gene_names = list(aggregated_gene_embeddings.keys())\n",
    "gene_embeddings_array = np.stack(list(aggregated_gene_embeddings.values()))\n",
    "\n",
    "print(f\"Embedding shape: {gene_embeddings_array.shape}\")\n",
    "\n",
    "# Visualize a subset of genes with UMAP\n",
    "n_genes_to_plot = min(1000, len(gene_names))\n",
    "gene_subset_idx = np.random.choice(len(gene_names), n_genes_to_plot, replace=False)\n",
    "gene_subset = gene_embeddings_array[gene_subset_idx]\n",
    "\n",
    "# UMAP for genes\n",
    "gene_reducer = umap.UMAP(min_dist=0.1, n_components=2, n_neighbors=15, random_state=42)\n",
    "gene_umap = gene_reducer.fit_transform(gene_subset)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(gene_umap[:, 0], gene_umap[:, 1], s=10, alpha=0.5, c='steelblue')\n",
    "plt.title(f'UMAP Visualization of {n_genes_to_plot} Gene Embeddings', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('UMAP 1', fontsize=12)\n",
    "plt.ylabel('UMAP 2', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nYou can access gene embeddings for a specific cell:\")\n",
    "print(f\"Example: gene_embeddings[0]['{gene_names[0]}']  # First cell, specific gene\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention-section",
   "metadata": {},
   "source": [
    "## Extract Attention Weights\n",
    "\n",
    "For interpretability, you can extract attention weights from the transformer layers. This requires using the PyTorch attention implementation instead of Flash Attention.\n",
    "\n",
    "**Note:** This is slower and uses more memory than the default Flash Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with torch attention implementation\n",
    "tahoe_config_attn = TahoeConfig(\n",
    "    model_size=\"70m\",\n",
    "    batch_size=4,  # Reduce batch size for memory efficiency\n",
    "    device=device,\n",
    "    attn_impl='torch'  # Required for attention extraction\n",
    ")\n",
    "\n",
    "tahoe_attn = Tahoe(configurer=tahoe_config_attn)\n",
    "print(\"Tahoe model with attention extraction loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-for-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a smaller subset for attention extraction\n",
    "ann_data_tiny = ann_data[:50]  # Use only 50 cells\n",
    "\n",
    "dataloader_attn = tahoe_attn.process_data(\n",
    "    ann_data_tiny,\n",
    "    gene_names=\"gene_name\",\n",
    "    use_raw_counts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention weights\n",
    "cell_embeddings_attn, attention_weights = tahoe_attn.get_embeddings(\n",
    "    dataloader_attn, \n",
    "    output_attentions=True\n",
    ")\n",
    "\n",
    "print(f\"Cell embeddings shape: {cell_embeddings_attn.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights dimensions: (n_cells, n_heads, seq_length, seq_length)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-attention-section",
   "metadata": {},
   "source": [
    "## Visualize Attention Patterns\n",
    "\n",
    "Let's visualize the attention pattern for one cell to see which genes the model pays attention to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first cell and average across attention heads\n",
    "cell_idx = 0\n",
    "cell_attention = attention_weights[cell_idx]  # Shape: (n_heads, seq_len, seq_len)\n",
    "\n",
    "# Average across heads\n",
    "avg_attention = cell_attention.mean(axis=0)  # Shape: (seq_len, seq_len)\n",
    "\n",
    "# Find actual sequence length (excluding padding)\n",
    "non_zero_mask = avg_attention.sum(axis=1) > 0\n",
    "actual_seq_len = non_zero_mask.sum()\n",
    "avg_attention_trimmed = avg_attention[:actual_seq_len, :actual_seq_len]\n",
    "\n",
    "# Plot attention heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    avg_attention_trimmed[:50, :50],  # Show first 50x50 for visibility\n",
    "    cmap='viridis',\n",
    "    square=True,\n",
    "    cbar_kws={'label': 'Attention Weight'}\n",
    ")\n",
    "plt.title(f'Attention Pattern for Cell {cell_idx} (averaged across heads)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Key Position (Gene Tokens)', fontsize=12)\n",
    "plt.ylabel('Query Position (Gene Tokens)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Showing first 50x50 positions of {actual_seq_len} total sequence length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7bwwq6axah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transformer embeddings with their corresponding gene IDs\n",
    "# This ensures we know which embedding corresponds to which gene\n",
    "transformer_embs, gene_ids = tahoe.get_transformer_embeddings(dataloader)\n",
    "\n",
    "print(f\"Number of cells: {len(transformer_embs)}\")\n",
    "print(f\"First cell embeddings shape: {transformer_embs[0].shape}\")\n",
    "print(f\"First cell gene IDs shape: {gene_ids[0].shape}\")\n",
    "print(f\"Embedding dimension: {transformer_embs[0].shape[1]}\")\n",
    "\n",
    "# Show the gene mapping for first cell\n",
    "pad_token_id = tahoe.collator_cfg[\"pad_token_id\"]\n",
    "idx_to_gene = tahoe.vocab.index_to_token\n",
    "\n",
    "print(f\"\\nFirst cell - first 5 non-padding genes:\")\n",
    "count = 0\n",
    "for pos in range(len(gene_ids[0])):\n",
    "    if gene_ids[0][pos] != pad_token_id and count < 5:\n",
    "        gene_name = idx_to_gene[gene_ids[0][pos]]\n",
    "        print(f\"  Position {pos}: {gene_name} (ID: {gene_ids[0][pos]})\")\n",
    "        count += 1\n",
    "\n",
    "# Optional: Modify embeddings for perturbation experiments\n",
    "# Example - perturb gene at position 5 in first cell\n",
    "# transformer_embs[0][5, :] += np.random.randn(transformer_embs[0].shape[1]) * 0.1\n",
    "\n",
    "# Decode embeddings to predicted expression\n",
    "# Returns a list of pandas Series (one per cell)\n",
    "expr_predictions = tahoe.decode_embeddings(transformer_embs, gene_ids)\n",
    "\n",
    "print(f\"\\nNumber of cells with predictions: {len(expr_predictions)}\")\n",
    "print(f\"First cell has predictions for {len(expr_predictions[0])} genes\")\n",
    "\n",
    "# Show predictions for first cell, first 5 genes\n",
    "print(f\"\\nFirst cell - predicted expression for first 5 genes:\")\n",
    "for gene_name, pred_expr in list(expr_predictions[0].items())[:5]:\n",
    "    print(f\"  {gene_name}: {pred_expr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9rl846l5ph9",
   "metadata": {},
   "source": [
    "## Expression Prediction with Decoder\n",
    "\n",
    "Tahoe includes an expression decoder that can predict gene expression values from embeddings. This is useful for:\n",
    "- **In-silico perturbation**: Modify gene embeddings and predict resulting expression changes\n",
    "- **Counterfactual analysis**: Answer \"what if\" questions about gene regulation\n",
    "- **Expression imputation**: Predict expression from partial or modified embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tauvtjy6cz8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. ✅ Load and configure the Tahoe-x1 model for single-cell RNA-seq analysis\n",
    "2. ✅ Process scRNA-seq data with automatic gene symbol to Ensembl ID mapping\n",
    "3. ✅ Extract cell embeddings that capture cellular states\n",
    "4. ✅ Extract gene embeddings per cell (list of pandas Series, one per cell)\n",
    "5. ✅ Visualize embeddings using UMAP for exploratory analysis\n",
    "6. ✅ Extract and visualize attention weights for model interpretability\n",
    "7. ✅ Use the expression decoder for predicting expression from embeddings\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Cell embeddings**: Dense vector representations capturing cellular transcriptional states (numpy array)\n",
    "- **Gene embeddings**: List of pandas Series, one per cell. Each Series contains gene embeddings indexed by Ensembl IDs for genes expressed in that cell\n",
    "- **Attention weights**: Interpretable attention patterns (requires `attn_impl='torch'`)\n",
    "- **Expression decoder**: Predict gene expression values from embeddings (useful for perturbation experiments)\n",
    "\n",
    "### Gene Embeddings Structure\n",
    "\n",
    "```python\n",
    "# gene_embeddings is a list with length = number of cells\n",
    "len(gene_embeddings)  # e.g., 500 cells\n",
    "\n",
    "# Each element is a pandas Series for that cell\n",
    "gene_embeddings[0]  # pandas Series with gene IDs as keys\n",
    "\n",
    "# Access specific gene in specific cell\n",
    "gene_embeddings[0]['ENSG00000123456']  # numpy array of shape (embedding_dim,)\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Cell Type Annotation**: Use the embeddings for downstream tasks like cell type classification\n",
    "- **Gene Analysis**: Analyze gene expression patterns using per-cell gene embeddings\n",
    "- **Perturbation Experiments**: Use the decoder to predict expression changes from modified embeddings\n",
    "- **Integration**: Combine Tahoe embeddings with other analysis tools in the scRNA-seq ecosystem\n",
    "- **Fine-tuning**: Adapt the model for specific downstream tasks (see other notebooks)\n",
    "\n",
    "### Model Information\n",
    "\n",
    "- **Model**: Tahoe-x1 by Tahoe Therapeutics\n",
    "- **Hugging Face**: [tahoebio/Tahoe-x1](https://huggingface.co/tahoebio/Tahoe-x1)\n",
    "- **Architecture**: Transformer-based foundation model for scRNA-seq\n",
    "- **Available sizes**: 70m (12 layers, 512d), 1b (24 layers, 1024d), 3b (36 layers, 1536d)\n",
    "\n",
    "For more information and examples, visit the [Helical documentation](https://helical.readthedocs.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
